{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-Net: Convolutional networks for biomedical image segmentation (O. Ronneberger et al., 2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch-lightning        1.5.0\n",
      "torch                    1.11.0\n",
      "torchmetrics             0.5.1\n",
      "torchvision              0.12.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lrobaina/school/IMPACT-MRI/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pytorch_lightning as pl\n",
    "from fastmri.data.subsample import create_mask_for_mask_type\n",
    "from fastmri.data.transforms import UnetDataTransform\n",
    "from fastmri.pl_modules import FastMriDataModule, UnetModule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Space Mask for transforming the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_types = [\n",
    "    \"random\",\n",
    "    \"equispaced\",\n",
    "    \"equispaced_fraction\",\n",
    "    \"magic\",\n",
    "    \"magic_fraction\"\n",
    "]\n",
    "mask_type = mask_types[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of center lines to use in mask\n",
    "center_fractions = [0.09]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acceleration rates to use for masks\n",
    "accelerations = [4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fastmri.data.subsample.RandomMaskFunc"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = create_mask_for_mask_type(\n",
    "    mask_type, center_fractions, accelerations\n",
    ")\n",
    "type(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use random masks for train transform, fixed masks for val transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data specific Parameters\n",
    "data_path = Path('../data/singlecoil_datasets/')\n",
    "test_path = Path('../data/singlecoil_datasets/singlecoil_test/')\n",
    "challenge = \"singlecoil\"\n",
    "test_split = \"test\"\n",
    "# Fraction of slices in the dataset to use (train split only). \n",
    "# If not given all will be used. Cannot set together with volume_sample_rate.\n",
    "sample_rate = None\n",
    "val_sample_rate = None\n",
    "test_sample_rate = None\n",
    "volume_sample_rate = None\n",
    "val_volume_sample_rate = None\n",
    "test_volume_sample_rate = None\n",
    "use_dataset_cache_file = True\n",
    "combine_train_val = False\n",
    "\n",
    "# data loader arguments\n",
    "batch_size = 1\n",
    "num_workers = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<fastmri.data.transforms.UnetDataTransform at 0x7f07a0b5c460>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_transform = UnetDataTransform(challenge, mask_func=mask, use_seed=False)\n",
    "train_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_transform = UnetDataTransform(challenge, mask_func=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform = UnetDataTransform(challenge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'singlecoil'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_module = FastMriDataModule(\n",
    "        data_path=data_path,\n",
    "        challenge=challenge,\n",
    "        train_transform=train_transform,\n",
    "        val_transform=val_transform,\n",
    "        test_transform=test_transform,\n",
    "        test_split=test_split,\n",
    "        test_path=test_path,\n",
    "        sample_rate=sample_rate,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        distributed_sampler=False,\n",
    ")\n",
    "data_module.challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "# UNet Model Hyperparameters #\n",
    "##############################\n",
    "in_chans=1          # number of input channels to U-Net\n",
    "out_chans=1         # number of output chanenls to U-Net\n",
    "chans=32            # number of top-level U-Net channels\n",
    "num_pool_layers=4   # number of U-Net pooling layers\n",
    "drop_prob=0.0       # dropout probability\n",
    "lr=0.001            # RMSProp learning rate\n",
    "lr_step_size=40     # epoch at which to decrease learning rate\n",
    "lr_gamma=0.1        # extent to which to decrease learning rate\n",
    "weight_decay=0.0    # weight decay regularization strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UnetModule(\n",
    "        in_chans=in_chans,\n",
    "        out_chans=out_chans,\n",
    "        chans=chans,\n",
    "        num_pool_layers=num_pool_layers,\n",
    "        drop_prob=drop_prob,\n",
    "        lr=lr,\n",
    "        lr_step_size=lr_step_size,\n",
    "        lr_gamma=lr_gamma,\n",
    "        weight_decay=weight_decay,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_config = dict(\n",
    "    #gpus=1,                     # number of gpus to use\n",
    "    #replace_sampler_ddp=False,  # this is necessary for volume dispatch during val\n",
    "    strategy=None,               # what distributed version to use\n",
    "    #seed=42,                    # random seed\n",
    "    deterministic=True,         # makes things slower, but deterministic\n",
    "    default_root_dir='../logs',  # directory for logs and checkpoints\n",
    "    max_epochs=50,              # max number of epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(**trainer_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name             | Type                 | Params\n",
      "----------------------------------------------------------\n",
      "0 | NMSE             | DistributedMetricSum | 0     \n",
      "1 | SSIM             | DistributedMetricSum | 0     \n",
      "2 | PSNR             | DistributedMetricSum | 0     \n",
      "3 | ValLoss          | DistributedMetricSum | 0     \n",
      "4 | TotExamples      | DistributedMetricSum | 0     \n",
      "5 | TotSliceExamples | DistributedMetricSum | 0     \n",
      "6 | unet             | Unet                 | 7.8 M \n",
      "----------------------------------------------------------\n",
      "7.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "7.8 M     Total params\n",
      "31.024    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lrobaina/school/IMPACT-MRI/.venv/lib/python3.10/site-packages/pytorch_lightning/utilities/data.py:56: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n",
      "/home/lrobaina/school/IMPACT-MRI/fastmri/pl_modules/unet_module.py:104: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1, 320, 320])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  \"val_loss\": F.l1_loss(output, batch.target),\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input images must have the same dimensions.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(model, datamodule\u001b[39m=\u001b[39;49mdata_module)\n",
      "File \u001b[0;32m~/school/IMPACT-MRI/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:735\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, train_dataloader, ckpt_path)\u001b[0m\n\u001b[1;32m    730\u001b[0m     rank_zero_deprecation(\n\u001b[1;32m    731\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`trainer.fit(train_dataloader)` is deprecated in v1.4 and will be removed in v1.6.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    732\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m Use `trainer.fit(train_dataloaders)` instead. HINT: added \u001b[39m\u001b[39m'\u001b[39m\u001b[39ms\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    733\u001b[0m     )\n\u001b[1;32m    734\u001b[0m     train_dataloaders \u001b[39m=\u001b[39m train_dataloader\n\u001b[0;32m--> 735\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    736\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    737\u001b[0m )\n",
      "File \u001b[0;32m~/school/IMPACT-MRI/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:682\u001b[0m, in \u001b[0;36mTrainer._call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    673\u001b[0m \u001b[39mError handling, intended to be used only for main trainer function entry points (fit, validate, test, predict)\u001b[39;00m\n\u001b[1;32m    674\u001b[0m \u001b[39mas all errors should funnel through them\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[39m    **kwargs: keyword arguments to be passed to `trainer_fn`\u001b[39;00m\n\u001b[1;32m    680\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    681\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 682\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    683\u001b[0m \u001b[39m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001b[39;00m\n\u001b[1;32m    684\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m \u001b[39mas\u001b[39;00m exception:\n",
      "File \u001b[0;32m~/school/IMPACT-MRI/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:770\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    768\u001b[0m \u001b[39m# TODO: ckpt_path only in v1.7\u001b[39;00m\n\u001b[1;32m    769\u001b[0m ckpt_path \u001b[39m=\u001b[39m ckpt_path \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresume_from_checkpoint\n\u001b[0;32m--> 770\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[1;32m    772\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    773\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/school/IMPACT-MRI/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1193\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheckpoint_connector\u001b[39m.\u001b[39mresume_end()\n\u001b[1;32m   1192\u001b[0m \u001b[39m# dispatch `start_training` or `start_evaluating` or `start_predicting`\u001b[39;00m\n\u001b[0;32m-> 1193\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch()\n\u001b[1;32m   1195\u001b[0m \u001b[39m# plugin will finalized fitting (e.g. ddp_spawn will load trained model)\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_post_dispatch()\n",
      "File \u001b[0;32m~/school/IMPACT-MRI/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1272\u001b[0m, in \u001b[0;36mTrainer._dispatch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_type_plugin\u001b[39m.\u001b[39mstart_predicting(\u001b[39mself\u001b[39m)\n\u001b[1;32m   1271\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1272\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_type_plugin\u001b[39m.\u001b[39;49mstart_training(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[0;32m~/school/IMPACT-MRI/.venv/lib/python3.10/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py:202\u001b[0m, in \u001b[0;36mTrainingTypePlugin.start_training\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstart_training\u001b[39m(\u001b[39mself\u001b[39m, trainer: \u001b[39m\"\u001b[39m\u001b[39mpl.Trainer\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m     \u001b[39m# double dispatch to initiate the training loop\u001b[39;00m\n\u001b[0;32m--> 202\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_results \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mrun_stage()\n",
      "File \u001b[0;32m~/school/IMPACT-MRI/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1282\u001b[0m, in \u001b[0;36mTrainer.run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1280\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[1;32m   1281\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_predict()\n\u001b[0;32m-> 1282\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_train()\n",
      "File \u001b[0;32m~/school/IMPACT-MRI/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1304\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1301\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_global_zero \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprogress_bar_callback \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1302\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprogress_bar_callback\u001b[39m.\u001b[39mdisable()\n\u001b[0;32m-> 1304\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_sanity_check(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlightning_module)\n\u001b[1;32m   1306\u001b[0m \u001b[39m# enable train mode\u001b[39;00m\n\u001b[1;32m   1307\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/school/IMPACT-MRI/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1368\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self, ref_model)\u001b[0m\n\u001b[1;32m   1366\u001b[0m \u001b[39m# run eval step\u001b[39;00m\n\u001b[1;32m   1367\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m-> 1368\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluation_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m   1370\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcall_hook(\u001b[39m\"\u001b[39m\u001b[39mon_sanity_check_end\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1372\u001b[0m \u001b[39m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m~/school/IMPACT-MRI/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py:145\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 145\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    146\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    147\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrestarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/school/IMPACT-MRI/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py:109\u001b[0m, in \u001b[0;36mEvaluationLoop.advance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_fetcher \u001b[39m=\u001b[39m dataloader \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39m_data_connector\u001b[39m.\u001b[39mget_profiled_dataloader(\n\u001b[1;32m    105\u001b[0m     dataloader, dataloader_idx\u001b[39m=\u001b[39mdataloader_idx\n\u001b[1;32m    106\u001b[0m )\n\u001b[1;32m    107\u001b[0m dl_max_batches \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_batches[dataloader_idx]\n\u001b[0;32m--> 109\u001b[0m dl_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(dataloader, dataloader_idx, dl_max_batches, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_dataloaders)\n\u001b[1;32m    111\u001b[0m \u001b[39m# store batch level output per dataloader\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutputs\u001b[39m.\u001b[39mappend(dl_outputs)\n",
      "File \u001b[0;32m~/school/IMPACT-MRI/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py:145\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 145\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    146\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    147\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrestarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/school/IMPACT-MRI/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:124\u001b[0m, in \u001b[0;36mEvaluationEpochLoop.advance\u001b[0;34m(self, data_fetcher, dataloader_idx, dl_max_batches, num_dataloaders)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mevaluation_step_and_end\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    123\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_evaluation_step(batch, batch_idx, dataloader_idx)\n\u001b[0;32m--> 124\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluation_step_end(output)\n\u001b[1;32m    126\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_processed()\n\u001b[1;32m    128\u001b[0m \u001b[39m# track loss history\u001b[39;00m\n",
      "File \u001b[0;32m~/school/IMPACT-MRI/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:222\u001b[0m, in \u001b[0;36mEvaluationEpochLoop._evaluation_step_end\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Calls the `{validation/test}_step_end` hook.\"\"\"\u001b[39;00m\n\u001b[1;32m    221\u001b[0m hook_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtest_step_end\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mtesting \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mvalidation_step_end\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 222\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mcall_hook(hook_name, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    223\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/school/IMPACT-MRI/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1483\u001b[0m, in \u001b[0;36mTrainer.call_hook\u001b[0;34m(self, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1481\u001b[0m model_fx \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(pl_module, hook_name, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m   1482\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcallable\u001b[39m(model_fx):\n\u001b[0;32m-> 1483\u001b[0m     output \u001b[39m=\u001b[39m model_fx(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1485\u001b[0m \u001b[39m# *Bad code alert*\u001b[39;00m\n\u001b[1;32m   1486\u001b[0m \u001b[39m# The `Accelerator` mostly calls the `TrainingTypePlugin` but some of those calls are deprecated.\u001b[39;00m\n\u001b[1;32m   1487\u001b[0m \u001b[39m# The following logic selectively chooses which hooks are called on each object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1491\u001b[0m \n\u001b[1;32m   1492\u001b[0m \u001b[39m# call the accelerator hook\u001b[39;00m\n\u001b[1;32m   1493\u001b[0m \u001b[39mif\u001b[39;00m hook_name \u001b[39min\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39mon_train_start\u001b[39m\u001b[39m\"\u001b[39m,) \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator, hook_name):\n",
      "File \u001b[0;32m~/school/IMPACT-MRI/fastmri/pl_modules/mri_module.py:139\u001b[0m, in \u001b[0;36mMriModule.validation_step_end\u001b[0;34m(self, val_logs)\u001b[0m\n\u001b[1;32m    132\u001b[0m     mse_vals[fname][slice_num] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(\n\u001b[1;32m    133\u001b[0m         evaluate\u001b[39m.\u001b[39mmse(target, output)\n\u001b[1;32m    134\u001b[0m     )\u001b[39m.\u001b[39mview(\u001b[39m1\u001b[39m)\n\u001b[1;32m    135\u001b[0m     target_norms[fname][slice_num] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(\n\u001b[1;32m    136\u001b[0m         evaluate\u001b[39m.\u001b[39mmse(target, np\u001b[39m.\u001b[39mzeros_like(target))\n\u001b[1;32m    137\u001b[0m     )\u001b[39m.\u001b[39mview(\u001b[39m1\u001b[39m)\n\u001b[1;32m    138\u001b[0m     ssim_vals[fname][slice_num] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(\n\u001b[0;32m--> 139\u001b[0m         evaluate\u001b[39m.\u001b[39;49mssim(target[\u001b[39mNone\u001b[39;49;00m, \u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m], output[\u001b[39mNone\u001b[39;49;00m, \u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m], maxval\u001b[39m=\u001b[39;49mmaxval)\n\u001b[1;32m    140\u001b[0m     )\u001b[39m.\u001b[39mview(\u001b[39m1\u001b[39m)\n\u001b[1;32m    141\u001b[0m     max_vals[fname] \u001b[39m=\u001b[39m maxval\n\u001b[1;32m    143\u001b[0m \u001b[39mreturn\u001b[39;00m {\n\u001b[1;32m    144\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m\"\u001b[39m: val_logs[\u001b[39m\"\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    145\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mmse_vals\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mdict\u001b[39m(mse_vals),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mmax_vals\u001b[39m\u001b[39m\"\u001b[39m: max_vals,\n\u001b[1;32m    149\u001b[0m }\n",
      "File \u001b[0;32m~/school/IMPACT-MRI/fastmri/evaluate.py:53\u001b[0m, in \u001b[0;36mssim\u001b[0;34m(gt, pred, maxval)\u001b[0m\n\u001b[1;32m     51\u001b[0m ssim \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([\u001b[39m0\u001b[39m])\n\u001b[1;32m     52\u001b[0m \u001b[39mfor\u001b[39;00m slice_num \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(gt\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]):\n\u001b[0;32m---> 53\u001b[0m     ssim \u001b[39m=\u001b[39m ssim \u001b[39m+\u001b[39m structural_similarity(\n\u001b[1;32m     54\u001b[0m         gt[slice_num], pred[slice_num], data_range\u001b[39m=\u001b[39;49mmaxval\n\u001b[1;32m     55\u001b[0m     )\n\u001b[1;32m     57\u001b[0m \u001b[39mreturn\u001b[39;00m ssim \u001b[39m/\u001b[39m gt\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/school/IMPACT-MRI/.venv/lib/python3.10/site-packages/skimage/metrics/_structural_similarity.py:111\u001b[0m, in \u001b[0;36mstructural_similarity\u001b[0;34m(im1, im2, win_size, gradient, data_range, channel_axis, gaussian_weights, full, **kwargs)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstructural_similarity\u001b[39m(im1, im2,\n\u001b[1;32m     16\u001b[0m                           \u001b[39m*\u001b[39m,\n\u001b[1;32m     17\u001b[0m                           win_size\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, gradient\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, data_range\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     18\u001b[0m                           channel_axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     19\u001b[0m                           gaussian_weights\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, full\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     20\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[39m    Compute the mean structural similarity index between two images.\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[39m    Please pay attention to the `data_range` parameter with floating-point images.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    109\u001b[0m \n\u001b[1;32m    110\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m     check_shape_equality(im1, im2)\n\u001b[1;32m    112\u001b[0m     float_type \u001b[39m=\u001b[39m _supported_float_type(im1\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m    114\u001b[0m     \u001b[39mif\u001b[39;00m channel_axis \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    115\u001b[0m         \u001b[39m# loop over channels\u001b[39;00m\n",
      "File \u001b[0;32m~/school/IMPACT-MRI/.venv/lib/python3.10/site-packages/skimage/_shared/utils.py:504\u001b[0m, in \u001b[0;36mcheck_shape_equality\u001b[0;34m(*images)\u001b[0m\n\u001b[1;32m    502\u001b[0m image0 \u001b[39m=\u001b[39m images[\u001b[39m0\u001b[39m]\n\u001b[1;32m    503\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mall\u001b[39m(image0\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m image\u001b[39m.\u001b[39mshape \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images[\u001b[39m1\u001b[39m:]):\n\u001b[0;32m--> 504\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mInput images must have the same dimensions.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    505\u001b[0m \u001b[39mreturn\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Input images must have the same dimensions."
     ]
    }
   ],
   "source": [
    "trainer.fit(model, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
