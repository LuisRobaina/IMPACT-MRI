{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains a PyTorch implementation and code for running pretrained models based on the paper:\n",
    "\n",
    "U-Net: Convolutional networks for biomedical image segmentation (O. Ronneberger et al., 2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1615,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pytorch_lightning as pl\n",
    "from fastmri.data.subsample import create_mask_for_mask_type\n",
    "from fastmri.data.transforms import UnetDataTransform\n",
    "from fastmri.pl_modules import FastMriDataModule, UnetModule\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from fastmri.models import Unet\n",
    "\n",
    "import pathlib\n",
    "from argparse import ArgumentParser\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from torch import nn\n",
    "from torchmetrics.metric import Metric\n",
    "\n",
    "import fastmri\n",
    "from fastmri import evaluate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code below was copied over from the fastmri github in order to modify UNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1616,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DistributedMetricSum(Metric):\n",
    "    def __init__(self, dist_sync_on_step=True):\n",
    "        super().__init__(dist_sync_on_step=dist_sync_on_step)\n",
    "\n",
    "        self.add_state(\"quantity\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n",
    "\n",
    "    def update(self, batch: torch.Tensor):  # type: ignore\n",
    "        self.quantity += batch\n",
    "\n",
    "    def compute(self):\n",
    "        return self.quantity\n",
    "\n",
    "\n",
    "class MriModule(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Abstract super class for deep larning reconstruction models.\n",
    "\n",
    "    This is a subclass of the LightningModule class from pytorch_lightning,\n",
    "    with some additional functionality specific to fastMRI:\n",
    "        - Evaluating reconstructions\n",
    "        - Visualization\n",
    "\n",
    "    To implement a new reconstruction model, inherit from this class and\n",
    "    implement the following methods:\n",
    "        - training_step, validation_step, test_step:\n",
    "            Define what happens in one step of training, validation, and\n",
    "            testing, respectively\n",
    "        - configure_optimizers:\n",
    "            Create and return the optimizers\n",
    "\n",
    "    Other methods from LightningModule can be overridden as needed.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_log_images: int = 16):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_log_images: Number of images to log. Defaults to 16.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_log_images = num_log_images\n",
    "        self.val_log_indices = None\n",
    "\n",
    "        self.NMSE = DistributedMetricSum()\n",
    "        self.SSIM = DistributedMetricSum()\n",
    "        self.PSNR = DistributedMetricSum()\n",
    "        self.ValLoss = DistributedMetricSum()\n",
    "        self.TotExamples = DistributedMetricSum()\n",
    "        self.TotSliceExamples = DistributedMetricSum()\n",
    "\n",
    "    def validation_step_end(self, val_logs):\n",
    "        # check inputs\n",
    "        for k in (\n",
    "            \"batch_idx\",\n",
    "            \"fname\",\n",
    "            \"slice_num\",\n",
    "            \"max_value\",\n",
    "            \"output\",\n",
    "            \"target\",\n",
    "            \"val_loss\",\n",
    "        ):\n",
    "            if k not in val_logs.keys():\n",
    "                raise RuntimeError(\n",
    "                    f\"Expected key {k} in dict returned by validation_step.\"\n",
    "                )\n",
    "        if val_logs[\"output\"].ndim == 2:\n",
    "            val_logs[\"output\"] = val_logs[\"output\"].unsqueeze(0)\n",
    "        elif val_logs[\"output\"].ndim != 3:\n",
    "            raise RuntimeError(\"Unexpected output size from validation_step.\")\n",
    "        if val_logs[\"target\"].ndim == 2:\n",
    "            val_logs[\"target\"] = val_logs[\"target\"].unsqueeze(0)\n",
    "        elif val_logs[\"target\"].ndim != 3:\n",
    "            raise RuntimeError(\"Unexpected output size from validation_step.\")\n",
    "\n",
    "        # pick a set of images to log if we don't have one already\n",
    "        if self.val_log_indices is None:\n",
    "            self.val_log_indices = list(\n",
    "                np.random.permutation(len(self.trainer.val_dataloaders[0]))[\n",
    "                    : self.num_log_images\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        # log images to tensorboard\n",
    "        if isinstance(val_logs[\"batch_idx\"], int):\n",
    "            batch_indices = [val_logs[\"batch_idx\"]]\n",
    "        else:\n",
    "            batch_indices = val_logs[\"batch_idx\"]\n",
    "        for i, batch_idx in enumerate(batch_indices):\n",
    "            if batch_idx in self.val_log_indices:\n",
    "                key = f\"val_images_idx_{batch_idx}\"\n",
    "                target = val_logs[\"target\"][i].unsqueeze(0)\n",
    "                output = val_logs[\"output\"][i].unsqueeze(0)\n",
    "                error = torch.abs(target - output)\n",
    "                output = output / output.max()\n",
    "                target = target / target.max()\n",
    "                error = error / error.max()\n",
    "                self.log_image(f\"{key}/target\", target)\n",
    "                self.log_image(f\"{key}/reconstruction\", output)\n",
    "                self.log_image(f\"{key}/error\", error)\n",
    "\n",
    "        # compute evaluation metrics\n",
    "        mse_vals = defaultdict(dict)\n",
    "        target_norms = defaultdict(dict)\n",
    "        ssim_vals = defaultdict(dict)\n",
    "        max_vals = dict()\n",
    "        for i, fname in enumerate(val_logs[\"fname\"]):\n",
    "            slice_num = int(val_logs[\"slice_num\"][i].cpu())\n",
    "            maxval = val_logs[\"max_value\"][i].cpu().numpy()\n",
    "            output = val_logs[\"output\"][i].cpu().numpy()\n",
    "            target = val_logs[\"target\"][i].cpu().numpy()\n",
    "\n",
    "            mse_vals[fname][slice_num] = torch.tensor(\n",
    "                evaluate.mse(target, output)\n",
    "            ).view(1)\n",
    "            target_norms[fname][slice_num] = torch.tensor(\n",
    "                evaluate.mse(target, np.zeros_like(target))\n",
    "            ).view(1)\n",
    "            ssim_vals[fname][slice_num] = torch.tensor(\n",
    "                evaluate.ssim(target[None, ...], output[None, ...], maxval=maxval)\n",
    "            ).view(1)\n",
    "            max_vals[fname] = maxval\n",
    "\n",
    "        return {\n",
    "            \"val_loss\": val_logs[\"val_loss\"],\n",
    "            \"mse_vals\": dict(mse_vals),\n",
    "            \"target_norms\": dict(target_norms),\n",
    "            \"ssim_vals\": dict(ssim_vals),\n",
    "            \"max_vals\": max_vals,\n",
    "        }\n",
    "\n",
    "    def log_image(self, name, image):\n",
    "        self.logger.experiment.add_image(name, image, global_step=self.global_step)\n",
    "\n",
    "    def validation_epoch_end(self, val_logs):\n",
    "        # aggregate losses\n",
    "        losses = []\n",
    "        mse_vals = defaultdict(dict)\n",
    "        target_norms = defaultdict(dict)\n",
    "        ssim_vals = defaultdict(dict)\n",
    "        max_vals = dict()\n",
    "\n",
    "        # use dict updates to handle duplicate slices\n",
    "        for val_log in val_logs:\n",
    "            losses.append(val_log[\"val_loss\"].view(-1))\n",
    "\n",
    "            for k in val_log[\"mse_vals\"].keys():\n",
    "                mse_vals[k].update(val_log[\"mse_vals\"][k])\n",
    "            for k in val_log[\"target_norms\"].keys():\n",
    "                target_norms[k].update(val_log[\"target_norms\"][k])\n",
    "            for k in val_log[\"ssim_vals\"].keys():\n",
    "                ssim_vals[k].update(val_log[\"ssim_vals\"][k])\n",
    "            for k in val_log[\"max_vals\"]:\n",
    "                max_vals[k] = val_log[\"max_vals\"][k]\n",
    "\n",
    "        # check to make sure we have all files in all metrics\n",
    "        assert (\n",
    "            mse_vals.keys()\n",
    "            == target_norms.keys()\n",
    "            == ssim_vals.keys()\n",
    "            == max_vals.keys()\n",
    "        )\n",
    "\n",
    "        # apply means across image volumes\n",
    "        metrics = {\"nmse\": 0, \"ssim\": 0, \"psnr\": 0}\n",
    "        local_examples = 0\n",
    "        for fname in mse_vals.keys():\n",
    "            local_examples = local_examples + 1\n",
    "            mse_val = torch.mean(\n",
    "                torch.cat([v.view(-1) for _, v in mse_vals[fname].items()])\n",
    "            )\n",
    "            target_norm = torch.mean(\n",
    "                torch.cat([v.view(-1) for _, v in target_norms[fname].items()])\n",
    "            )\n",
    "            metrics[\"nmse\"] = metrics[\"nmse\"] + mse_val / target_norm\n",
    "            metrics[\"psnr\"] = (\n",
    "                metrics[\"psnr\"]\n",
    "                + 20\n",
    "                * torch.log10(\n",
    "                    torch.tensor(\n",
    "                        max_vals[fname], dtype=mse_val.dtype, device=mse_val.device\n",
    "                    )\n",
    "                )\n",
    "                - 10 * torch.log10(mse_val)\n",
    "            )\n",
    "            metrics[\"ssim\"] = metrics[\"ssim\"] + torch.mean(\n",
    "                torch.cat([v.view(-1) for _, v in ssim_vals[fname].items()])\n",
    "            )\n",
    "\n",
    "        # reduce across ddp via sum\n",
    "        metrics[\"nmse\"] = self.NMSE(metrics[\"nmse\"])\n",
    "        metrics[\"ssim\"] = self.SSIM(metrics[\"ssim\"])\n",
    "        metrics[\"psnr\"] = self.PSNR(metrics[\"psnr\"])\n",
    "        tot_examples = self.TotExamples(torch.tensor(local_examples))\n",
    "        val_loss = self.ValLoss(torch.sum(torch.cat(losses)))\n",
    "        tot_slice_examples = self.TotSliceExamples(\n",
    "            torch.tensor(len(losses), dtype=torch.float)\n",
    "        )\n",
    "\n",
    "        self.log(\"validation_loss\", val_loss / tot_slice_examples, prog_bar=True)\n",
    "        for metric, value in metrics.items():\n",
    "            self.log(f\"val_metrics/{metric}\", value / tot_examples)\n",
    "\n",
    "    def test_epoch_end(self, test_logs):\n",
    "        outputs = defaultdict(dict)\n",
    "\n",
    "        # use dicts for aggregation to handle duplicate slices in ddp mode\n",
    "        for log in test_logs:\n",
    "            for i, (fname, slice_num) in enumerate(zip(log[\"fname\"], log[\"slice\"])):\n",
    "                outputs[fname][int(slice_num.cpu())] = log[\"output\"][i]\n",
    "\n",
    "        # stack all the slices for each file\n",
    "        for fname in outputs:\n",
    "            outputs[fname] = np.stack(\n",
    "                [out for _, out in sorted(outputs[fname].items())]\n",
    "            )\n",
    "\n",
    "        # pull the default_root_dir if we have a trainer, otherwise save to cwd\n",
    "        if hasattr(self, \"trainer\"):\n",
    "            save_path = pathlib.Path(self.trainer.default_root_dir) / \"reconstructions\"\n",
    "        else:\n",
    "            save_path = pathlib.Path.cwd() / \"reconstructions\"\n",
    "        self.print(f\"Saving reconstructions to {save_path}\")\n",
    "\n",
    "        fastmri.save_reconstructions(outputs, save_path)\n",
    "\n",
    "    @staticmethod\n",
    "    def add_model_specific_args(parent_parser):  # pragma: no-cover\n",
    "        \"\"\"\n",
    "        Define parameters that only apply to this model\n",
    "        \"\"\"\n",
    "        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n",
    "\n",
    "        # logging params\n",
    "        parser.add_argument(\n",
    "            \"--num_log_images\",\n",
    "            default=16,\n",
    "            type=int,\n",
    "            help=\"Number of images to log to Tensorboard\",\n",
    "        )\n",
    "\n",
    "        return parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1617,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetModule(MriModule):\n",
    "    \"\"\"\n",
    "    Unet training module.\n",
    "\n",
    "    This can be used to train baseline U-Nets from the paper:\n",
    "\n",
    "    J. Zbontar et al. fastMRI: An Open Dataset and Benchmarks for Accelerated\n",
    "    MRI. arXiv:1811.08839. 2018.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_chans=1,\n",
    "        out_chans=1,\n",
    "        chans=32,\n",
    "        num_pool_layers=4,\n",
    "        drop_prob=0.0,\n",
    "        lr=0.001,\n",
    "        lr_step_size=40,\n",
    "        lr_gamma=0.1,\n",
    "        weight_decay=0.0,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_chans (int, optional): Number of channels in the input to the\n",
    "                U-Net model. Defaults to 1.\n",
    "            out_chans (int, optional): Number of channels in the output to the\n",
    "                U-Net model. Defaults to 1.\n",
    "            chans (int, optional): Number of output channels of the first\n",
    "                convolution layer. Defaults to 32.\n",
    "            num_pool_layers (int, optional): Number of down-sampling and\n",
    "                up-sampling layers. Defaults to 4.\n",
    "            drop_prob (float, optional): Dropout probability. Defaults to 0.0.\n",
    "            lr (float, optional): Learning rate. Defaults to 0.001.\n",
    "            lr_step_size (int, optional): Learning rate step size. Defaults to\n",
    "                40.\n",
    "            lr_gamma (float, optional): Learning rate gamma decay. Defaults to\n",
    "                0.1.\n",
    "            weight_decay (float, optional): Parameter for penalizing weights\n",
    "                norm. Defaults to 0.0.\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.out_chans = out_chans\n",
    "        self.chans = chans\n",
    "        self.num_pool_layers = num_pool_layers\n",
    "        self.drop_prob = drop_prob\n",
    "        self.lr = lr\n",
    "        self.lr_step_size = lr_step_size\n",
    "        self.lr_gamma = lr_gamma\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "        self.unet = Unet(\n",
    "            in_chans=self.in_chans,\n",
    "            out_chans=self.out_chans,\n",
    "            chans=self.chans,\n",
    "            num_pool_layers=self.num_pool_layers,\n",
    "            drop_prob=self.drop_prob,\n",
    "        )\n",
    "\n",
    "    def forward(self, image):\n",
    "        return self.unet(image.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        output = self(batch.image)\n",
    "        loss = F.l1_loss(output, batch.target)\n",
    "\n",
    "        self.log(\"loss\", loss.detach())\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        output = self(batch.image)\n",
    "        mean = batch.mean.unsqueeze(1).unsqueeze(2)\n",
    "        std = batch.std.unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        return {\n",
    "            \"batch_idx\": batch_idx,\n",
    "            \"fname\": batch.fname,\n",
    "            \"slice_num\": batch.slice_num,\n",
    "            \"max_value\": batch.max_value,\n",
    "            \"output\": output * std + mean,\n",
    "            \"target\": batch.target * std + mean,\n",
    "            \"val_loss\": F.l1_loss(output, batch.target),\n",
    "        }\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        output = self.forward(batch.image)\n",
    "        mean = batch.mean.unsqueeze(1).unsqueeze(2)\n",
    "        std = batch.std.unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        return {\n",
    "            \"fname\": batch.fname,\n",
    "            \"slice\": batch.slice_num,\n",
    "            \"output\": (output * std + mean).cpu().numpy(),\n",
    "        }\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.RMSprop(\n",
    "            self.parameters(),\n",
    "            lr=self.lr,\n",
    "            weight_decay=self.weight_decay,\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "            optim, self.lr_step_size, self.lr_gamma\n",
    "        )\n",
    "\n",
    "        return [optim], [scheduler]\n",
    "\n",
    "    @staticmethod\n",
    "    def add_model_specific_args(parent_parser):  # pragma: no-cover\n",
    "        \"\"\"\n",
    "        Define parameters that only apply to this model\n",
    "        \"\"\"\n",
    "        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n",
    "        parser = MriModule.add_model_specific_args(parser)\n",
    "\n",
    "        # network params\n",
    "        parser.add_argument(\n",
    "            \"--in_chans\", default=1, type=int, help=\"Number of U-Net input channels\"\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--out_chans\", default=1, type=int, help=\"Number of U-Net output chanenls\"\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--chans\", default=1, type=int, help=\"Number of top-level U-Net filters.\"\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--num_pool_layers\",\n",
    "            default=4,\n",
    "            type=int,\n",
    "            help=\"Number of U-Net pooling layers.\",\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--drop_prob\", default=0.0, type=float, help=\"U-Net dropout probability\"\n",
    "        )\n",
    "\n",
    "        # training params (opt)\n",
    "        parser.add_argument(\n",
    "            \"--lr\", default=0.001, type=float, help=\"RMSProp learning rate\"\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--lr_step_size\",\n",
    "            default=40,\n",
    "            type=int,\n",
    "            help=\"Epoch at which to decrease step size\",\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--lr_gamma\", default=0.1, type=float, help=\"Amount to decrease step size\"\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--weight_decay\",\n",
    "            default=0.0,\n",
    "            type=float,\n",
    "            help=\"Strength of weight decay regularization\",\n",
    "        )\n",
    "\n",
    "        return parser\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unet with Attention Gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1618,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_chans: int, out_chans: int, drop_prob: float):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.out_chans = out_chans\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(in_chans, out_chans, kernel_size=3, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(out_chans),\n",
    "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "            nn.Dropout2d(drop_prob),\n",
    "            nn.Conv2d(out_chans, out_chans, kernel_size=3, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(out_chans),\n",
    "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "            nn.Dropout2d(drop_prob),\n",
    "        )\n",
    " \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1619,
   "metadata": {},
   "outputs": [],
   "source": [
    "class attention_gate(nn.Module):\n",
    "    \"\"\"SOURCES: https://arxiv.org/pdf/1804.03999.pdf\n",
    "                https://idiotdeveloper.com/attention-unet-in-pytorch/\n",
    "       AG is characterised by a set of parameters Θatt containing: linear transformations Wx ∈ Fl×Fint,\n",
    "       Wg ∈ Fg×Fint, ψ ∈ Fint×1 and bias terms. The linear transformations are computed using\n",
    "       channel-wise 1x1x1 convolutions for the input tensors. In other contexts [33], this is referred to as\n",
    "       vector concatenation-based attention, where the concatenated features x and g are linearly mapped\n",
    "       to a Fint dimensional intermediate space.\"\"\"\n",
    "    def __init__(self, F_g, F_l, F_int):\n",
    "        super().__init__()\n",
    " \n",
    "        self.Wg = nn.Sequential(\n",
    "            nn.Conv2d(F_g, F_int, kernel_size=1, padding=0),\n",
    "            nn.InstanceNorm2d(F_int),\n",
    "        )\n",
    "        self.Wx = nn.Sequential(\n",
    "            nn.Conv2d(F_l, F_int, kernel_size=1, padding=0),\n",
    "            nn.InstanceNorm2d(F_int),\n",
    "        )\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv2d(F_int, 1, kernel_size=1,padding=0),\n",
    "            nn.InstanceNorm2d(1),\n",
    "        )\n",
    "        \n",
    "        self.Sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, g, x):\n",
    "        #g is the output from convolution\n",
    "        #x is the skip connection with corresponding dimensions in the downsample\n",
    "        Wg = self.Wg(g)\n",
    "        Wx = self.Wx(x)\n",
    "        alpha = self.relu(Wg + Wx)\n",
    "        alpha = self.psi(alpha)\n",
    "        alpha = self.Sigmoid(alpha)\n",
    "        return x * alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1620,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransposeConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A Transpose Convolutional Block that consists of one convolution transpose\n",
    "    layers followed by instance normalization and LeakyReLU activation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_chans: int, out_chans: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_chans: Number of channels in the input.\n",
    "            out_chans: Number of channels in the output.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.out_chans = out_chans\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_chans, out_chans, kernel_size=2, stride=2, bias=False\n",
    "            ),\n",
    "            nn.InstanceNorm2d(out_chans),\n",
    "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, image: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image: Input 4D tensor of shape `(N, in_chans, H, W)`.\n",
    "\n",
    "        Returns:\n",
    "            Output tensor of shape `(N, out_chans, H*2, W*2)`.\n",
    "        \"\"\"\n",
    "        return self.layers(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1621,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet(nn.Module):\n",
    "    \"\"\"\n",
    "    PyTorch implementation of a U-Net model.\n",
    "\n",
    "    O. Ronneberger, P. Fischer, and Thomas Brox. U-net: Convolutional networks\n",
    "    for biomedical image segmentation. In International Conference on Medical\n",
    "    image computing and computer-assisted intervention, pages 234–241.\n",
    "    Springer, 2015.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_chans: int,\n",
    "        out_chans: int,\n",
    "        chans: int = 32,\n",
    "        num_pool_layers: int = 4,\n",
    "        drop_prob: float = 0.0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_chans: Number of channels in the input to the U-Net model.\n",
    "            out_chans: Number of channels in the output to the U-Net model.\n",
    "            chans: Number of output channels of the first convolution layer.\n",
    "            num_pool_layers: Number of down-sampling and up-sampling layers.\n",
    "            drop_prob: Dropout probability.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.out_chans = out_chans\n",
    "        self.chans = chans\n",
    "        self.num_pool_layers = num_pool_layers\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "        self.down_sample_layers = nn.ModuleList([ConvBlock(in_chans, chans, drop_prob)])\n",
    "        ch = chans\n",
    "        for _ in range(num_pool_layers - 1):\n",
    "            self.down_sample_layers.append(ConvBlock(ch, ch * 2, drop_prob))\n",
    "            ch *= 2\n",
    "        self.conv = ConvBlock(ch, ch * 2, drop_prob)\n",
    "\n",
    "        self.up_conv = nn.ModuleList()\n",
    "        self.up_transpose_conv = nn.ModuleList()\n",
    "        \n",
    "        #setup list of attention gates\n",
    "        self.attention_gates = nn.ModuleList()\n",
    "        \n",
    "        for _ in range(num_pool_layers - 1):\n",
    "            self.up_transpose_conv.append(TransposeConvBlock(ch * 2, ch))\n",
    "            self.up_conv.append(ConvBlock(ch * 2, ch, drop_prob))\n",
    "            \n",
    "            #append attention_gates into list\n",
    "            self.attention_gates.append(attention_gate(ch, ch, ch//2))\n",
    "            ch //= 2\n",
    "\n",
    "        self.up_transpose_conv.append(TransposeConvBlock(ch * 2, ch))\n",
    "        self.up_conv.append(\n",
    "            nn.Sequential(\n",
    "                ConvBlock(ch * 2, ch, drop_prob),\n",
    "                nn.Conv2d(ch, self.out_chans, kernel_size=1, stride=1),\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        #append one more attention_gate into list\n",
    "        self.attention_gates.append(attention_gate(ch, ch, ch//2))\n",
    "        \n",
    "    def forward(self, image: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image: Input 4D tensor of shape `(N, in_chans, H, W)`.\n",
    "\n",
    "        Returns:\n",
    "            Output tensor of shape `(N, out_chans, H, W)`.\n",
    "        \"\"\"\n",
    "        stack = []\n",
    "        output = image\n",
    "\n",
    "        # apply down-sampling layers\n",
    "        for layer in self.down_sample_layers:\n",
    "            output = layer(output)\n",
    "            stack.append(output)\n",
    "            output = F.avg_pool2d(output, kernel_size=2, stride=2, padding=0)\n",
    "\n",
    "        output = self.conv(output)\n",
    "\n",
    "        # apply up-sampling layers\n",
    "        for transpose_conv, conv, attention in zip(self.up_transpose_conv, self.up_conv, self.attention_gates):\n",
    "            downsample_layer = stack.pop()\n",
    "            output = transpose_conv(output)\n",
    "            \n",
    "            #calculate attention using the output from previous layer and skip connection layer\n",
    "            downsample_layer = attention(output, downsample_layer)\n",
    "            \n",
    "            # reflect pad on the right/botton if needed to handle odd input dimensions\n",
    "            padding = [0, 0, 0, 0]\n",
    "            if output.shape[-1] != downsample_layer.shape[-1]:\n",
    "                padding[1] = 1  # padding right\n",
    "            if output.shape[-2] != downsample_layer.shape[-2]:\n",
    "                padding[3] = 1  # padding bottom\n",
    "            if torch.sum(torch.tensor(padding)) != 0:\n",
    "                output = F.pad(output, padding, \"reflect\")\n",
    "            \n",
    "            output = torch.cat([output, downsample_layer], dim=1)\n",
    "            output = conv(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Space Mask for transforming the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1622,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_types = [\n",
    "    \"random\",\n",
    "    \"equispaced\",\n",
    "    \"equispaced_fraction\",\n",
    "    \"magic\",\n",
    "    \"magic_fraction\"\n",
    "]\n",
    "mask_type = mask_types[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1623,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of center lines to use in mask\n",
    "center_fractions = [0.09]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1624,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acceleration rates to use for masks\n",
    "accelerations = [4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1625,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fastmri.data.subsample.RandomMaskFunc"
      ]
     },
     "execution_count": 1625,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = create_mask_for_mask_type(\n",
    "    mask_type, center_fractions, accelerations\n",
    ")\n",
    "type(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use random masks for train transform, fixed masks for val transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1626,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data specific Parameters\n",
    "data_path = Path('../data/')\n",
    "test_path = Path('../data/singlecoil_test')\n",
    "challenge = \"singlecoil\"\n",
    "test_split = \"test\"\n",
    "# Fraction of slices in the dataset to use (train split only). \n",
    "# If not given all will be used. Cannot set together with volume_sample_rate.\n",
    "sample_rate = None\n",
    "val_sample_rate = None\n",
    "test_sample_rate = None\n",
    "volume_sample_rate = None\n",
    "val_volume_sample_rate = None\n",
    "test_volume_sample_rate = None\n",
    "use_dataset_cache_file = True\n",
    "combine_train_val = False\n",
    "\n",
    "# data loader arguments\n",
    "batch_size = 1\n",
    "num_workers = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1627,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<fastmri.data.transforms.UnetDataTransform at 0x2c4b7f280>"
      ]
     },
     "execution_count": 1627,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_transform = UnetDataTransform(challenge, mask_func=mask, use_seed=False)\n",
    "train_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1628,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_transform = UnetDataTransform(challenge, mask_func=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1629,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform = UnetDataTransform(challenge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1630,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'singlecoil'"
      ]
     },
     "execution_count": 1630,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_module = FastMriDataModule(\n",
    "        data_path=data_path,\n",
    "        challenge=challenge,\n",
    "        train_transform=train_transform,\n",
    "        val_transform=val_transform,\n",
    "        test_transform=test_transform,\n",
    "        test_split=test_split,\n",
    "        test_path=test_path,\n",
    "        sample_rate=sample_rate,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        distributed_sampler=None,\n",
    ")\n",
    "data_module.challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1631,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify access to datasets is ready...\n",
    "data_module.prepare_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1632,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "# UNet Model Hyperparameters #\n",
    "##############################\n",
    "in_chans=1          # number of input channels to U-Net\n",
    "out_chans=1         # number of output chanenls to U-Net\n",
    "chans=32            # number of top-level U-Net channels\n",
    "num_pool_layers=4   # number of U-Net pooling layers\n",
    "drop_prob=0.0       # dropout probability\n",
    "lr=0.001            # RMSProp learning rate\n",
    "lr_step_size=40     # epoch at which to decrease learning rate\n",
    "lr_gamma=0.1        # extent to which to decrease learning rate\n",
    "weight_decay=0.0    # weight decay regularization strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1633,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UnetModule(\n",
    "        in_chans=in_chans,\n",
    "        out_chans=out_chans,\n",
    "        chans=chans,\n",
    "        num_pool_layers=num_pool_layers,\n",
    "        drop_prob=drop_prob,\n",
    "        lr=lr,\n",
    "        lr_step_size=lr_step_size,\n",
    "        lr_gamma=lr_gamma,\n",
    "        weight_decay=weight_decay,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1634,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_config = dict(\n",
    "    #replace_sampler_ddp=False,  # this is necessary for volume dispatch during val\n",
    "    #strategy=\"ddp\",               # what distributed version to use\n",
    "    #seed=42,   # random seed\n",
    "    accelerator = \"cpu\",\n",
    "    devices=1,                     # number of gpus to use\n",
    "    deterministic=True,         # makes things slower, but deterministic\n",
    "    default_root_dir='../logs',  # directory for logs and checkpoints\n",
    "    max_epochs=10,              # max number of epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1635,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(**trainer_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1636,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name             | Type                 | Params\n",
      "----------------------------------------------------------\n",
      "0 | NMSE             | DistributedMetricSum | 0     \n",
      "1 | SSIM             | DistributedMetricSum | 0     \n",
      "2 | PSNR             | DistributedMetricSum | 0     \n",
      "3 | ValLoss          | DistributedMetricSum | 0     \n",
      "4 | TotExamples      | DistributedMetricSum | 0     \n",
      "5 | TotSliceExamples | DistributedMetricSum | 0     \n",
      "6 | unet             | Unet                 | 7.8 M \n",
      "----------------------------------------------------------\n",
      "7.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "7.8 M     Total params\n",
      "31.375    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|                                        | 0/41877 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
