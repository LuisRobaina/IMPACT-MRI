{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U-Net: Convolutional networks for biomedical image segmentation (O. Ronneberger et al., 2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pytorch_lightning as pl\n",
    "from fastmri.data.subsample import create_mask_for_mask_type\n",
    "from fastmri.data.transforms import UnetDataTransform\n",
    "from fastmri.pl_modules import FastMriDataModule, UnetModule\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import pathlib\n",
    "from argparse import ArgumentParser\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from torch import nn\n",
    "from torchmetrics.metric import Metric\n",
    "\n",
    "import fastmri\n",
    "from fastmri.models.unet import ConvBlock, TransposeConvBlock\n",
    "from fastmri import evaluate\n",
    "from fastmri.pl_modules import MriModule\n",
    "from fastmri.pl_modules.mri_module import DistributedMetricSum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## code below was adapted from the fastmri github in order to modify the UNET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unet with Attention Gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class attention_gate(nn.Module):\n",
    "    \"\"\"SOURCES: https://arxiv.org/pdf/1804.03999.pdf\n",
    "                https://idiotdeveloper.com/attention-unet-in-pytorch/\n",
    "       AG is characterised by a set of parameters Θatt containing: linear transformations Wx ∈ Fl×Fint,\n",
    "       Wg ∈ Fg×Fint, ψ ∈ Fint×1 and bias terms. The linear transformations are computed using\n",
    "       channel-wise 1x1x1 convolutions for the input tensors. In other contexts [33], this is referred to as\n",
    "       vector concatenation-based attention, where the concatenated features x and g are linearly mapped\n",
    "       to a Fint dimensional intermediate space.\"\"\"\n",
    "    def __init__(self, F_g, F_l, F_int):\n",
    "        super().__init__()\n",
    " \n",
    "        self.Wg = nn.Sequential(\n",
    "            nn.Conv2d(F_g, F_int, kernel_size=1, padding=0),\n",
    "            nn.InstanceNorm2d(F_int),\n",
    "        )\n",
    "        self.Wx = nn.Sequential(\n",
    "            nn.Conv2d(F_l, F_int, kernel_size=1, padding=0),\n",
    "            nn.InstanceNorm2d(F_int),\n",
    "        )\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv2d(F_int, 1, kernel_size=1,padding=0),\n",
    "            nn.InstanceNorm2d(1),\n",
    "        )\n",
    "        \n",
    "        self.Sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, g, x):\n",
    "        #g is the output from convolution\n",
    "        #x is the skip connection with corresponding dimensions in the downsample\n",
    "        Wg = self.Wg(g)\n",
    "        Wx = self.Wx(x)\n",
    "        alpha = self.relu(Wg + Wx)\n",
    "        alpha = self.psi(alpha)\n",
    "        alpha = self.Sigmoid(alpha)\n",
    "        return x * alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionUnet(nn.Module):\n",
    "    \"\"\"\n",
    "    PyTorch implementation of a U-Net model with Attention.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_chans: int,\n",
    "        out_chans: int,\n",
    "        chans: int = 32,\n",
    "        num_pool_layers: int = 4,\n",
    "        drop_prob: float = 0.0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_chans: Number of channels in the input to the U-Net model.\n",
    "            out_chans: Number of channels in the output to the U-Net model.\n",
    "            chans: Number of output channels of the first convolution layer.\n",
    "            num_pool_layers: Number of down-sampling and up-sampling layers.\n",
    "            drop_prob: Dropout probability.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.out_chans = out_chans\n",
    "        self.chans = chans\n",
    "        self.num_pool_layers = num_pool_layers\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "        self.down_sample_layers = nn.ModuleList([ConvBlock(in_chans, chans, drop_prob)])\n",
    "\n",
    "        ch = chans\n",
    "        \n",
    "        for _ in range(num_pool_layers - 1):\n",
    "            self.down_sample_layers.append(ConvBlock(ch, ch * 2, drop_prob))\n",
    "            ch *= 2\n",
    "        \n",
    "        self.conv = ConvBlock(ch, ch * 2, drop_prob)\n",
    "\n",
    "        self.up_conv = nn.ModuleList()\n",
    "        self.up_transpose_conv = nn.ModuleList()\n",
    "        \n",
    "        #setup list of attention gates\n",
    "        self.attention_gates = nn.ModuleList()\n",
    "        \n",
    "        for _ in range(num_pool_layers - 1):\n",
    "            self.up_transpose_conv.append(TransposeConvBlock(ch * 2, ch))\n",
    "            self.up_conv.append(ConvBlock(ch * 2, ch, drop_prob))\n",
    "            \n",
    "            #append attention_gates into list\n",
    "            self.attention_gates.append(attention_gate(ch, ch, ch//2))\n",
    "            ch //= 2\n",
    "\n",
    "        self.up_transpose_conv.append(TransposeConvBlock(ch * 2, ch))\n",
    "        self.up_conv.append(\n",
    "            nn.Sequential(\n",
    "                ConvBlock(ch * 2, ch, drop_prob),\n",
    "                nn.Conv2d(ch, self.out_chans, kernel_size=1, stride=1),\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        #append one more attention_gate into list\n",
    "        self.attention_gates.append(attention_gate(ch, ch, ch//2))\n",
    "        \n",
    "    def forward(self, image: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image: Input 4D tensor of shape `(N, in_chans, H, W)`.\n",
    "\n",
    "        Returns:\n",
    "            Output tensor of shape `(N, out_chans, H, W)`.\n",
    "        \"\"\"\n",
    "        stack = []\n",
    "        output = image\n",
    "\n",
    "        # apply down-sampling layers\n",
    "        for layer in self.down_sample_layers:\n",
    "            output = layer(output)\n",
    "            stack.append(output)\n",
    "            output = F.avg_pool2d(output, kernel_size=2, stride=2, padding=0)\n",
    "\n",
    "        output = self.conv(output)\n",
    "\n",
    "        # apply up-sampling layers\n",
    "        for transpose_conv, conv, attention in zip(self.up_transpose_conv, self.up_conv, self.attention_gates):\n",
    "            downsample_layer = stack.pop()\n",
    "            output = transpose_conv(output)\n",
    "            \n",
    "            #calculate attention using the output from previous layer and skip connection layer\n",
    "            downsample_layer = attention(output, downsample_layer)\n",
    "            \n",
    "            # reflect pad on the right/botton if needed to handle odd input dimensions\n",
    "            padding = [0, 0, 0, 0]\n",
    "            if output.shape[-1] != downsample_layer.shape[-1]:\n",
    "                padding[1] = 1  # padding right\n",
    "            if output.shape[-2] != downsample_layer.shape[-2]:\n",
    "                padding[3] = 1  # padding bottom\n",
    "            if torch.sum(torch.tensor(padding)) != 0:\n",
    "                output = F.pad(output, padding, \"reflect\")\n",
    "            \n",
    "            output = torch.cat([output, downsample_layer], dim=1)\n",
    "            output = conv(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Space Mask for transforming the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_types = [\n",
    "    \"random\",\n",
    "    \"equispaced\",\n",
    "    \"equispaced_fraction\",\n",
    "    \"magic\",\n",
    "    \"magic_fraction\"\n",
    "]\n",
    "mask_type = mask_types[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of center lines to use in mask\n",
    "center_fractions = [0.09]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acceleration rates to use for masks\n",
    "accelerations = [4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fastmri.data.subsample.RandomMaskFunc"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = create_mask_for_mask_type(\n",
    "    mask_type, center_fractions, accelerations\n",
    ")\n",
    "type(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data specific Parameters\n",
    "data_path = Path('../data/')\n",
    "test_path = Path('../data/singlecoil_test')\n",
    "challenge = \"singlecoil\"\n",
    "test_split = \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fraction of slices in the dataset to use (train split only). \n",
    "# If not given all will be used. Cannot set together with volume_sample_rate.\n",
    "sample_rate = None\n",
    "val_sample_rate = None\n",
    "test_sample_rate = None\n",
    "volume_sample_rate = None\n",
    "val_volume_sample_rate = None\n",
    "test_volume_sample_rate = None\n",
    "use_dataset_cache_file = True\n",
    "combine_train_val = False\n",
    "\n",
    "# data loader arguments\n",
    "batch_size = 1\n",
    "num_workers = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use random masks for train transform, fixed masks for val transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<fastmri.data.transforms.UnetDataTransform at 0x7fea435f9b40>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_transform = UnetDataTransform(challenge, mask_func=mask, use_seed=False)\n",
    "train_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_transform = UnetDataTransform(challenge, mask_func=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform = UnetDataTransform(challenge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'singlecoil'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_module = FastMriDataModule(\n",
    "        data_path=data_path,\n",
    "        challenge=challenge,\n",
    "        train_transform=train_transform,\n",
    "        val_transform=val_transform,\n",
    "        test_transform=test_transform,\n",
    "        test_split=test_split,\n",
    "        test_path=test_path,\n",
    "        sample_rate=sample_rate,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        distributed_sampler=None,\n",
    ")\n",
    "data_module.challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/singlecoil_train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Verify access to datasets is ready...\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m data_module\u001b[39m.\u001b[39;49mprepare_data()\n",
      "File \u001b[0;32m~/school/IMPACT-MRI/.venv/lib/python3.10/site-packages/pytorch_lightning/core/datamodule.py:474\u001b[0m, in \u001b[0;36mLightningDataModule._track_data_hook_calls.<locals>.wrapped_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    469\u001b[0m     rank_zero_deprecation(\n\u001b[1;32m    470\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDataModule.\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m has already been called, so it will not be called again. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    471\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIn v1.6 this behavior will change to always call DataModule.\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    472\u001b[0m     )\n\u001b[1;32m    473\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 474\u001b[0m     fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/school/IMPACT-MRI/fastmri/pl_modules/data_module.py:309\u001b[0m, in \u001b[0;36mFastMriDataModule.prepare_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    307\u001b[0m sample_rate \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample_rate  \u001b[39m# if i == 0 else 1.0\u001b[39;00m\n\u001b[1;32m    308\u001b[0m volume_sample_rate \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvolume_sample_rate  \u001b[39m# if i == 0 else None\u001b[39;00m\n\u001b[0;32m--> 309\u001b[0m _ \u001b[39m=\u001b[39m SliceDataset(\n\u001b[1;32m    310\u001b[0m     root\u001b[39m=\u001b[39;49mdata_path,\n\u001b[1;32m    311\u001b[0m     transform\u001b[39m=\u001b[39;49mdata_transform,\n\u001b[1;32m    312\u001b[0m     sample_rate\u001b[39m=\u001b[39;49msample_rate,\n\u001b[1;32m    313\u001b[0m     volume_sample_rate\u001b[39m=\u001b[39;49mvolume_sample_rate,\n\u001b[1;32m    314\u001b[0m     challenge\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchallenge,\n\u001b[1;32m    315\u001b[0m     use_dataset_cache\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49muse_dataset_cache_file,\n\u001b[1;32m    316\u001b[0m )\n",
      "File \u001b[0;32m~/school/IMPACT-MRI/fastmri/data/mri_data.py:299\u001b[0m, in \u001b[0;36mSliceDataset.__init__\u001b[0;34m(self, root, challenge, transform, use_dataset_cache, sample_rate, volume_sample_rate, dataset_cache_file, num_cols, raw_sample_filter)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[39m# check if our dataset is in the cache\u001b[39;00m\n\u001b[1;32m    297\u001b[0m \u001b[39m# if there, use that metadata, if not, then regenerate the metadata\u001b[39;00m\n\u001b[1;32m    298\u001b[0m \u001b[39mif\u001b[39;00m dataset_cache\u001b[39m.\u001b[39mget(root) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m use_dataset_cache:\n\u001b[0;32m--> 299\u001b[0m     files \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(Path(root)\u001b[39m.\u001b[39;49miterdir())\n\u001b[1;32m    300\u001b[0m     \u001b[39mfor\u001b[39;00m fname \u001b[39min\u001b[39;00m \u001b[39msorted\u001b[39m(files):\n\u001b[1;32m    301\u001b[0m         metadata, num_slices \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_retrieve_metadata(fname)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.10/lib/python3.10/pathlib.py:1017\u001b[0m, in \u001b[0;36mPath.iterdir\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1013\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39miterdir\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   1014\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Iterate over the files in this directory.  Does not yield any\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m \u001b[39m    result for the special paths '.' and '..'.\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1017\u001b[0m     \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_accessor\u001b[39m.\u001b[39;49mlistdir(\u001b[39mself\u001b[39;49m):\n\u001b[1;32m   1018\u001b[0m         \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m {\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m..\u001b[39m\u001b[39m'\u001b[39m}:\n\u001b[1;32m   1019\u001b[0m             \u001b[39m# Yielding a path object for these makes little sense\u001b[39;00m\n\u001b[1;32m   1020\u001b[0m             \u001b[39mcontinue\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/singlecoil_train'"
     ]
    }
   ],
   "source": [
    "# Verify access to datasets is ready...\n",
    "data_module.prepare_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "# UNet Model Hyperparameters #\n",
    "##############################\n",
    "in_chans=1          # number of input channels to U-Net\n",
    "out_chans=1         # number of output chanenls to U-Net\n",
    "chans=32            # number of top-level U-Net channels\n",
    "num_pool_layers=4   # number of U-Net pooling layers\n",
    "drop_prob=0.0       # dropout probability\n",
    "lr=0.001            # RMSProp learning rate\n",
    "lr_step_size=40     # epoch at which to decrease learning rate\n",
    "lr_gamma=0.1        # extent to which to decrease learning rate\n",
    "weight_decay=0.0    # weight decay regularization strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_model = UnetModule(\n",
    "        in_chans=in_chans,\n",
    "        out_chans=out_chans,\n",
    "        chans=chans,\n",
    "        num_pool_layers=num_pool_layers,\n",
    "        drop_prob=drop_prob,\n",
    "        lr=lr,\n",
    "        lr_step_size=lr_step_size,\n",
    "        lr_gamma=lr_gamma,\n",
    "        weight_decay=weight_decay,\n",
    ")\n",
    "att_unet_model = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_config = dict(\n",
    "    #replace_sampler_ddp=False,    # this is necessary for volume dispatch during val\n",
    "    #strategy=\"ddp\",               # what distributed version to use\n",
    "    #seed=42,                      # random seed\n",
    "    accelerator = \"cpu\",\n",
    "    devices=1,                     # number of gpus to use\n",
    "    deterministic=True,            # makes things slower, but deterministic\n",
    "    default_root_dir='../logs',    # directory for logs and checkpoints\n",
    "    max_epochs=10,                 # max number of epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(**trainer_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name             | Type                 | Params\n",
      "----------------------------------------------------------\n",
      "0 | NMSE             | DistributedMetricSum | 0     \n",
      "1 | SSIM             | DistributedMetricSum | 0     \n",
      "2 | PSNR             | DistributedMetricSum | 0     \n",
      "3 | ValLoss          | DistributedMetricSum | 0     \n",
      "4 | TotExamples      | DistributedMetricSum | 0     \n",
      "5 | TotSliceExamples | DistributedMetricSum | 0     \n",
      "6 | unet             | Unet                 | 7.8 M \n",
      "----------------------------------------------------------\n",
      "7.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "7.8 M     Total params\n",
      "31.375    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lrobaina/school/IMPACT-MRI/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/data_loading.py:110: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 87 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/lrobaina/school/IMPACT-MRI/.venv/lib/python3.10/site-packages/pytorch_lightning/utilities/data.py:56: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n",
      "/tmp/ipykernel_31798/1579936338.py:87: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1, 320, 320])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  \"val_loss\": F.l1_loss(output, batch.target),\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input images must have the same dimensions.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(model, datamodule\u001b[39m=\u001b[39;49mdata_module)\n",
      "File \u001b[0;32m~/school/IMPACT-MRI/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:735\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, train_dataloader, ckpt_path)\u001b[0m\n\u001b[1;32m    730\u001b[0m     rank_zero_deprecation(\n\u001b[1;32m    731\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`trainer.fit(train_dataloader)` is deprecated in v1.4 and will be removed in v1.6.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    732\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m Use `trainer.fit(train_dataloaders)` instead. HINT: added \u001b[39m\u001b[39m'\u001b[39m\u001b[39ms\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    733\u001b[0m     )\n\u001b[1;32m    734\u001b[0m     train_dataloaders \u001b[39m=\u001b[39m train_dataloader\n\u001b[0;32m--> 735\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    736\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    737\u001b[0m )\n",
      "File \u001b[0;32m~/school/IMPACT-MRI/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:682\u001b[0m, in \u001b[0;36mTrainer._call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    673\u001b[0m \u001b[39mError handling, intended to be used only for main trainer function entry points (fit, validate, test, predict)\u001b[39;00m\n\u001b[1;32m    674\u001b[0m \u001b[39mas all errors should funnel through them\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[39m    **kwargs: keyword arguments to be passed to `trainer_fn`\u001b[39;00m\n\u001b[1;32m    680\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    681\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 682\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    683\u001b[0m \u001b[39m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001b[39;00m\n\u001b[1;32m    684\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m \u001b[39mas\u001b[39;00m exception:\n",
      "File \u001b[0;32m~/school/IMPACT-MRI/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:770\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    768\u001b[0m \u001b[39m# TODO: ckpt_path only in v1.7\u001b[39;00m\n\u001b[1;32m    769\u001b[0m ckpt_path \u001b[39m=\u001b[39m ckpt_path \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresume_from_checkpoint\n\u001b[0;32m--> 770\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[1;32m    772\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    773\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/school/IMPACT-MRI/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1193\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheckpoint_connector\u001b[39m.\u001b[39mresume_end()\n\u001b[1;32m   1192\u001b[0m \u001b[39m# dispatch `start_training` or `start_evaluating` or `start_predicting`\u001b[39;00m\n\u001b[0;32m-> 1193\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch()\n\u001b[1;32m   1195\u001b[0m \u001b[39m# plugin will finalized fitting (e.g. ddp_spawn will load trained model)\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_post_dispatch()\n",
      "File \u001b[0;32m~/school/IMPACT-MRI/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1272\u001b[0m, in \u001b[0;36mTrainer._dispatch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_type_plugin\u001b[39m.\u001b[39mstart_predicting(\u001b[39mself\u001b[39m)\n\u001b[1;32m   1271\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1272\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_type_plugin\u001b[39m.\u001b[39;49mstart_training(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[0;32m~/school/IMPACT-MRI/.venv/lib/python3.10/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py:202\u001b[0m, in \u001b[0;36mTrainingTypePlugin.start_training\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstart_training\u001b[39m(\u001b[39mself\u001b[39m, trainer: \u001b[39m\"\u001b[39m\u001b[39mpl.Trainer\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m     \u001b[39m# double dispatch to initiate the training loop\u001b[39;00m\n\u001b[0;32m--> 202\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_results \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mrun_stage()\n",
      "File \u001b[0;32m~/school/IMPACT-MRI/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1282\u001b[0m, in \u001b[0;36mTrainer.run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1280\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[1;32m   1281\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_predict()\n\u001b[0;32m-> 1282\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_train()\n",
      "File \u001b[0;32m~/school/IMPACT-MRI/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1304\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1301\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_global_zero \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprogress_bar_callback \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1302\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprogress_bar_callback\u001b[39m.\u001b[39mdisable()\n\u001b[0;32m-> 1304\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_sanity_check(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlightning_module)\n\u001b[1;32m   1306\u001b[0m \u001b[39m# enable train mode\u001b[39;00m\n\u001b[1;32m   1307\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/school/IMPACT-MRI/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1368\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self, ref_model)\u001b[0m\n\u001b[1;32m   1366\u001b[0m \u001b[39m# run eval step\u001b[39;00m\n\u001b[1;32m   1367\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m-> 1368\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluation_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m   1370\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcall_hook(\u001b[39m\"\u001b[39m\u001b[39mon_sanity_check_end\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1372\u001b[0m \u001b[39m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m~/school/IMPACT-MRI/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py:145\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 145\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    146\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    147\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrestarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/school/IMPACT-MRI/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py:109\u001b[0m, in \u001b[0;36mEvaluationLoop.advance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_fetcher \u001b[39m=\u001b[39m dataloader \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39m_data_connector\u001b[39m.\u001b[39mget_profiled_dataloader(\n\u001b[1;32m    105\u001b[0m     dataloader, dataloader_idx\u001b[39m=\u001b[39mdataloader_idx\n\u001b[1;32m    106\u001b[0m )\n\u001b[1;32m    107\u001b[0m dl_max_batches \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_batches[dataloader_idx]\n\u001b[0;32m--> 109\u001b[0m dl_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(dataloader, dataloader_idx, dl_max_batches, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_dataloaders)\n\u001b[1;32m    111\u001b[0m \u001b[39m# store batch level output per dataloader\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutputs\u001b[39m.\u001b[39mappend(dl_outputs)\n",
      "File \u001b[0;32m~/school/IMPACT-MRI/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py:145\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 145\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    146\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    147\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrestarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/school/IMPACT-MRI/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:124\u001b[0m, in \u001b[0;36mEvaluationEpochLoop.advance\u001b[0;34m(self, data_fetcher, dataloader_idx, dl_max_batches, num_dataloaders)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mevaluation_step_and_end\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    123\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_evaluation_step(batch, batch_idx, dataloader_idx)\n\u001b[0;32m--> 124\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluation_step_end(output)\n\u001b[1;32m    126\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_processed()\n\u001b[1;32m    128\u001b[0m \u001b[39m# track loss history\u001b[39;00m\n",
      "File \u001b[0;32m~/school/IMPACT-MRI/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:222\u001b[0m, in \u001b[0;36mEvaluationEpochLoop._evaluation_step_end\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Calls the `{validation/test}_step_end` hook.\"\"\"\u001b[39;00m\n\u001b[1;32m    221\u001b[0m hook_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtest_step_end\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mtesting \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mvalidation_step_end\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 222\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mcall_hook(hook_name, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    223\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/school/IMPACT-MRI/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1483\u001b[0m, in \u001b[0;36mTrainer.call_hook\u001b[0;34m(self, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1481\u001b[0m model_fx \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(pl_module, hook_name, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m   1482\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcallable\u001b[39m(model_fx):\n\u001b[0;32m-> 1483\u001b[0m     output \u001b[39m=\u001b[39m model_fx(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1485\u001b[0m \u001b[39m# *Bad code alert*\u001b[39;00m\n\u001b[1;32m   1486\u001b[0m \u001b[39m# The `Accelerator` mostly calls the `TrainingTypePlugin` but some of those calls are deprecated.\u001b[39;00m\n\u001b[1;32m   1487\u001b[0m \u001b[39m# The following logic selectively chooses which hooks are called on each object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1491\u001b[0m \n\u001b[1;32m   1492\u001b[0m \u001b[39m# call the accelerator hook\u001b[39;00m\n\u001b[1;32m   1493\u001b[0m \u001b[39mif\u001b[39;00m hook_name \u001b[39min\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39mon_train_start\u001b[39m\u001b[39m\"\u001b[39m,) \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator, hook_name):\n",
      "Cell \u001b[0;32mIn[2], line 119\u001b[0m, in \u001b[0;36mMriModule.validation_step_end\u001b[0;34m(self, val_logs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     mse_vals[fname][slice_num] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(\n\u001b[1;32m    113\u001b[0m         evaluate\u001b[39m.\u001b[39mmse(target, output)\n\u001b[1;32m    114\u001b[0m     )\u001b[39m.\u001b[39mview(\u001b[39m1\u001b[39m)\n\u001b[1;32m    115\u001b[0m     target_norms[fname][slice_num] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(\n\u001b[1;32m    116\u001b[0m         evaluate\u001b[39m.\u001b[39mmse(target, np\u001b[39m.\u001b[39mzeros_like(target))\n\u001b[1;32m    117\u001b[0m     )\u001b[39m.\u001b[39mview(\u001b[39m1\u001b[39m)\n\u001b[1;32m    118\u001b[0m     ssim_vals[fname][slice_num] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(\n\u001b[0;32m--> 119\u001b[0m         evaluate\u001b[39m.\u001b[39;49mssim(target[\u001b[39mNone\u001b[39;49;00m, \u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m], output[\u001b[39mNone\u001b[39;49;00m, \u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m], maxval\u001b[39m=\u001b[39;49mmaxval)\n\u001b[1;32m    120\u001b[0m     )\u001b[39m.\u001b[39mview(\u001b[39m1\u001b[39m)\n\u001b[1;32m    121\u001b[0m     max_vals[fname] \u001b[39m=\u001b[39m maxval\n\u001b[1;32m    123\u001b[0m \u001b[39mreturn\u001b[39;00m {\n\u001b[1;32m    124\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m\"\u001b[39m: val_logs[\u001b[39m\"\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    125\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mmse_vals\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mdict\u001b[39m(mse_vals),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mmax_vals\u001b[39m\u001b[39m\"\u001b[39m: max_vals,\n\u001b[1;32m    129\u001b[0m }\n",
      "File \u001b[0;32m~/school/IMPACT-MRI/fastmri/evaluate.py:53\u001b[0m, in \u001b[0;36mssim\u001b[0;34m(gt, pred, maxval)\u001b[0m\n\u001b[1;32m     51\u001b[0m ssim \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([\u001b[39m0\u001b[39m])\n\u001b[1;32m     52\u001b[0m \u001b[39mfor\u001b[39;00m slice_num \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(gt\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]):\n\u001b[0;32m---> 53\u001b[0m     ssim \u001b[39m=\u001b[39m ssim \u001b[39m+\u001b[39m structural_similarity(\n\u001b[1;32m     54\u001b[0m         gt[slice_num], pred[slice_num], data_range\u001b[39m=\u001b[39;49mmaxval\n\u001b[1;32m     55\u001b[0m     )\n\u001b[1;32m     57\u001b[0m \u001b[39mreturn\u001b[39;00m ssim \u001b[39m/\u001b[39m gt\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/school/IMPACT-MRI/.venv/lib/python3.10/site-packages/skimage/metrics/_structural_similarity.py:111\u001b[0m, in \u001b[0;36mstructural_similarity\u001b[0;34m(im1, im2, win_size, gradient, data_range, channel_axis, gaussian_weights, full, **kwargs)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstructural_similarity\u001b[39m(im1, im2,\n\u001b[1;32m     16\u001b[0m                           \u001b[39m*\u001b[39m,\n\u001b[1;32m     17\u001b[0m                           win_size\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, gradient\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, data_range\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     18\u001b[0m                           channel_axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     19\u001b[0m                           gaussian_weights\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, full\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     20\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[39m    Compute the mean structural similarity index between two images.\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[39m    Please pay attention to the `data_range` parameter with floating-point images.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    109\u001b[0m \n\u001b[1;32m    110\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m     check_shape_equality(im1, im2)\n\u001b[1;32m    112\u001b[0m     float_type \u001b[39m=\u001b[39m _supported_float_type(im1\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m    114\u001b[0m     \u001b[39mif\u001b[39;00m channel_axis \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    115\u001b[0m         \u001b[39m# loop over channels\u001b[39;00m\n",
      "File \u001b[0;32m~/school/IMPACT-MRI/.venv/lib/python3.10/site-packages/skimage/_shared/utils.py:504\u001b[0m, in \u001b[0;36mcheck_shape_equality\u001b[0;34m(*images)\u001b[0m\n\u001b[1;32m    502\u001b[0m image0 \u001b[39m=\u001b[39m images[\u001b[39m0\u001b[39m]\n\u001b[1;32m    503\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mall\u001b[39m(image0\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m image\u001b[39m.\u001b[39mshape \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images[\u001b[39m1\u001b[39m:]):\n\u001b[0;32m--> 504\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mInput images must have the same dimensions.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    505\u001b[0m \u001b[39mreturn\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Input images must have the same dimensions."
     ]
    }
   ],
   "source": [
    "trainer.fit(model, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
