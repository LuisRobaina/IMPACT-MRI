{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains a PyTorch implementation and code for running pretrained models based on the paper:\n",
    "\n",
    "U-Net: Convolutional networks for biomedical image segmentation (O. Ronneberger et al., 2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lrobaina/school/IMPACT-MRI/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pytorch_lightning as pl\n",
    "from fastmri.data.subsample import create_mask_for_mask_type\n",
    "from fastmri.data.transforms import UnetDataTransform\n",
    "from fastmri.pl_modules import FastMriDataModule, UnetModule\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from fastmri.models import Unet\n",
    "\n",
    "import pathlib\n",
    "from argparse import ArgumentParser\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from torch import nn\n",
    "from torchmetrics.metric import Metric\n",
    "\n",
    "import fastmri\n",
    "from fastmri import evaluate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code below was copied over from the fastmri github in order to modify UNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistributedMetricSum(Metric):\n",
    "    def __init__(self, dist_sync_on_step=True):\n",
    "        super().__init__(dist_sync_on_step=dist_sync_on_step)\n",
    "\n",
    "        self.add_state(\"quantity\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n",
    "\n",
    "    def update(self, batch: torch.Tensor):  # type: ignore\n",
    "        self.quantity += batch\n",
    "\n",
    "    def compute(self):\n",
    "        return self.quantity\n",
    "\n",
    "\n",
    "class MriModule(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Abstract super class for deep larning reconstruction models.\n",
    "\n",
    "    This is a subclass of the LightningModule class from pytorch_lightning,\n",
    "    with some additional functionality specific to fastMRI:\n",
    "        - Evaluating reconstructions\n",
    "        - Visualization\n",
    "\n",
    "    To implement a new reconstruction model, inherit from this class and\n",
    "    implement the following methods:\n",
    "        - training_step, validation_step, test_step:\n",
    "            Define what happens in one step of training, validation, and\n",
    "            testing, respectively\n",
    "        - configure_optimizers:\n",
    "            Create and return the optimizers\n",
    "\n",
    "    Other methods from LightningModule can be overridden as needed.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_log_images: int = 16):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_log_images: Number of images to log. Defaults to 16.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_log_images = num_log_images\n",
    "        self.val_log_indices = None\n",
    "\n",
    "        self.NMSE = DistributedMetricSum()\n",
    "        self.SSIM = DistributedMetricSum()\n",
    "        self.PSNR = DistributedMetricSum()\n",
    "        self.ValLoss = DistributedMetricSum()\n",
    "        self.TotExamples = DistributedMetricSum()\n",
    "        self.TotSliceExamples = DistributedMetricSum()\n",
    "\n",
    "    def validation_step_end(self, val_logs):\n",
    "        # check inputs\n",
    "        for k in (\n",
    "            \"batch_idx\",\n",
    "            \"fname\",\n",
    "            \"slice_num\",\n",
    "            \"max_value\",\n",
    "            \"output\",\n",
    "            \"target\",\n",
    "            \"val_loss\",\n",
    "        ):\n",
    "            if k not in val_logs.keys():\n",
    "                raise RuntimeError(\n",
    "                    f\"Expected key {k} in dict returned by validation_step.\"\n",
    "                )\n",
    "        if val_logs[\"output\"].ndim == 2:\n",
    "            val_logs[\"output\"] = val_logs[\"output\"].unsqueeze(0)\n",
    "        elif val_logs[\"output\"].ndim != 3:\n",
    "            raise RuntimeError(\"Unexpected output size from validation_step.\")\n",
    "        if val_logs[\"target\"].ndim == 2:\n",
    "            val_logs[\"target\"] = val_logs[\"target\"].unsqueeze(0)\n",
    "        elif val_logs[\"target\"].ndim != 3:\n",
    "            raise RuntimeError(\"Unexpected output size from validation_step.\")\n",
    "\n",
    "        # pick a set of images to log if we don't have one already\n",
    "        if self.val_log_indices is None:\n",
    "            self.val_log_indices = list(\n",
    "                np.random.permutation(len(self.trainer.val_dataloaders[0]))[\n",
    "                    : self.num_log_images\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        # log images to tensorboard\n",
    "        if isinstance(val_logs[\"batch_idx\"], int):\n",
    "            batch_indices = [val_logs[\"batch_idx\"]]\n",
    "        else:\n",
    "            batch_indices = val_logs[\"batch_idx\"]\n",
    "        for i, batch_idx in enumerate(batch_indices):\n",
    "            if batch_idx in self.val_log_indices:\n",
    "                key = f\"val_images_idx_{batch_idx}\"\n",
    "                target = val_logs[\"target\"][i].unsqueeze(0)\n",
    "                output = val_logs[\"output\"][i].unsqueeze(0)\n",
    "                error = torch.abs(target - output)\n",
    "                output = output / output.max()\n",
    "                target = target / target.max()\n",
    "                error = error / error.max()\n",
    "                self.log_image(f\"{key}/target\", target)\n",
    "                self.log_image(f\"{key}/reconstruction\", output)\n",
    "                self.log_image(f\"{key}/error\", error)\n",
    "\n",
    "        # compute evaluation metrics\n",
    "        mse_vals = defaultdict(dict)\n",
    "        target_norms = defaultdict(dict)\n",
    "        ssim_vals = defaultdict(dict)\n",
    "        max_vals = dict()\n",
    "        for i, fname in enumerate(val_logs[\"fname\"]):\n",
    "            slice_num = int(val_logs[\"slice_num\"][i].cpu())\n",
    "            maxval = val_logs[\"max_value\"][i].cpu().numpy()\n",
    "            output = val_logs[\"output\"][i].cpu().numpy()\n",
    "            target = val_logs[\"target\"][i].cpu().numpy()\n",
    "\n",
    "            mse_vals[fname][slice_num] = torch.tensor(\n",
    "                evaluate.mse(target, output)\n",
    "            ).view(1)\n",
    "            target_norms[fname][slice_num] = torch.tensor(\n",
    "                evaluate.mse(target, np.zeros_like(target))\n",
    "            ).view(1)\n",
    "            ssim_vals[fname][slice_num] = torch.tensor(\n",
    "                evaluate.ssim(target[None, ...], output[None, ...], maxval=maxval)\n",
    "            ).view(1)\n",
    "            max_vals[fname] = maxval\n",
    "\n",
    "        return {\n",
    "            \"val_loss\": val_logs[\"val_loss\"],\n",
    "            \"mse_vals\": dict(mse_vals),\n",
    "            \"target_norms\": dict(target_norms),\n",
    "            \"ssim_vals\": dict(ssim_vals),\n",
    "            \"max_vals\": max_vals,\n",
    "        }\n",
    "\n",
    "    def log_image(self, name, image):\n",
    "        self.logger.experiment.add_image(name, image, global_step=self.global_step)\n",
    "\n",
    "    def validation_epoch_end(self, val_logs):\n",
    "        # aggregate losses\n",
    "        losses = []\n",
    "        mse_vals = defaultdict(dict)\n",
    "        target_norms = defaultdict(dict)\n",
    "        ssim_vals = defaultdict(dict)\n",
    "        max_vals = dict()\n",
    "\n",
    "        # use dict updates to handle duplicate slices\n",
    "        for val_log in val_logs:\n",
    "            losses.append(val_log[\"val_loss\"].view(-1))\n",
    "\n",
    "            for k in val_log[\"mse_vals\"].keys():\n",
    "                mse_vals[k].update(val_log[\"mse_vals\"][k])\n",
    "            for k in val_log[\"target_norms\"].keys():\n",
    "                target_norms[k].update(val_log[\"target_norms\"][k])\n",
    "            for k in val_log[\"ssim_vals\"].keys():\n",
    "                ssim_vals[k].update(val_log[\"ssim_vals\"][k])\n",
    "            for k in val_log[\"max_vals\"]:\n",
    "                max_vals[k] = val_log[\"max_vals\"][k]\n",
    "\n",
    "        # check to make sure we have all files in all metrics\n",
    "        assert (\n",
    "            mse_vals.keys()\n",
    "            == target_norms.keys()\n",
    "            == ssim_vals.keys()\n",
    "            == max_vals.keys()\n",
    "        )\n",
    "\n",
    "        # apply means across image volumes\n",
    "        metrics = {\"nmse\": 0, \"ssim\": 0, \"psnr\": 0}\n",
    "        local_examples = 0\n",
    "        for fname in mse_vals.keys():\n",
    "            local_examples = local_examples + 1\n",
    "            mse_val = torch.mean(\n",
    "                torch.cat([v.view(-1) for _, v in mse_vals[fname].items()])\n",
    "            )\n",
    "            target_norm = torch.mean(\n",
    "                torch.cat([v.view(-1) for _, v in target_norms[fname].items()])\n",
    "            )\n",
    "            metrics[\"nmse\"] = metrics[\"nmse\"] + mse_val / target_norm\n",
    "            metrics[\"psnr\"] = (\n",
    "                metrics[\"psnr\"]\n",
    "                + 20\n",
    "                * torch.log10(\n",
    "                    torch.tensor(\n",
    "                        max_vals[fname], dtype=mse_val.dtype, device=mse_val.device\n",
    "                    )\n",
    "                )\n",
    "                - 10 * torch.log10(mse_val)\n",
    "            )\n",
    "            metrics[\"ssim\"] = metrics[\"ssim\"] + torch.mean(\n",
    "                torch.cat([v.view(-1) for _, v in ssim_vals[fname].items()])\n",
    "            )\n",
    "\n",
    "        # reduce across ddp via sum\n",
    "        metrics[\"nmse\"] = self.NMSE(metrics[\"nmse\"])\n",
    "        metrics[\"ssim\"] = self.SSIM(metrics[\"ssim\"])\n",
    "        metrics[\"psnr\"] = self.PSNR(metrics[\"psnr\"])\n",
    "        tot_examples = self.TotExamples(torch.tensor(local_examples))\n",
    "        val_loss = self.ValLoss(torch.sum(torch.cat(losses)))\n",
    "        tot_slice_examples = self.TotSliceExamples(\n",
    "            torch.tensor(len(losses), dtype=torch.float)\n",
    "        )\n",
    "\n",
    "        self.log(\"validation_loss\", val_loss / tot_slice_examples, prog_bar=True)\n",
    "        for metric, value in metrics.items():\n",
    "            self.log(f\"val_metrics/{metric}\", value / tot_examples)\n",
    "\n",
    "    def test_epoch_end(self, test_logs):\n",
    "        outputs = defaultdict(dict)\n",
    "\n",
    "        # use dicts for aggregation to handle duplicate slices in ddp mode\n",
    "        for log in test_logs:\n",
    "            for i, (fname, slice_num) in enumerate(zip(log[\"fname\"], log[\"slice\"])):\n",
    "                outputs[fname][int(slice_num.cpu())] = log[\"output\"][i]\n",
    "\n",
    "        # stack all the slices for each file\n",
    "        for fname in outputs:\n",
    "            outputs[fname] = np.stack(\n",
    "                [out for _, out in sorted(outputs[fname].items())]\n",
    "            )\n",
    "\n",
    "        # pull the default_root_dir if we have a trainer, otherwise save to cwd\n",
    "        if hasattr(self, \"trainer\"):\n",
    "            save_path = pathlib.Path(self.trainer.default_root_dir) / \"reconstructions\"\n",
    "        else:\n",
    "            save_path = pathlib.Path.cwd() / \"reconstructions\"\n",
    "        self.print(f\"Saving reconstructions to {save_path}\")\n",
    "\n",
    "        fastmri.save_reconstructions(outputs, save_path)\n",
    "\n",
    "    @staticmethod\n",
    "    def add_model_specific_args(parent_parser):  # pragma: no-cover\n",
    "        \"\"\"\n",
    "        Define parameters that only apply to this model\n",
    "        \"\"\"\n",
    "        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n",
    "\n",
    "        # logging params\n",
    "        parser.add_argument(\n",
    "            \"--num_log_images\",\n",
    "            default=16,\n",
    "            type=int,\n",
    "            help=\"Number of images to log to Tensorboard\",\n",
    "        )\n",
    "\n",
    "        return parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetModule(MriModule):\n",
    "    \"\"\"\n",
    "    Unet training module.\n",
    "\n",
    "    This can be used to train baseline U-Nets from the paper:\n",
    "\n",
    "    J. Zbontar et al. fastMRI: An Open Dataset and Benchmarks for Accelerated\n",
    "    MRI. arXiv:1811.08839. 2018.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_chans=1,\n",
    "        out_chans=1,\n",
    "        chans=32,\n",
    "        num_pool_layers=4,\n",
    "        drop_prob=0.0,\n",
    "        lr=0.001,\n",
    "        lr_step_size=40,\n",
    "        lr_gamma=0.1,\n",
    "        weight_decay=0.0,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_chans (int, optional): Number of channels in the input to the\n",
    "                U-Net model. Defaults to 1.\n",
    "            out_chans (int, optional): Number of channels in the output to the\n",
    "                U-Net model. Defaults to 1.\n",
    "            chans (int, optional): Number of output channels of the first\n",
    "                convolution layer. Defaults to 32.\n",
    "            num_pool_layers (int, optional): Number of down-sampling and\n",
    "                up-sampling layers. Defaults to 4.\n",
    "            drop_prob (float, optional): Dropout probability. Defaults to 0.0.\n",
    "            lr (float, optional): Learning rate. Defaults to 0.001.\n",
    "            lr_step_size (int, optional): Learning rate step size. Defaults to\n",
    "                40.\n",
    "            lr_gamma (float, optional): Learning rate gamma decay. Defaults to\n",
    "                0.1.\n",
    "            weight_decay (float, optional): Parameter for penalizing weights\n",
    "                norm. Defaults to 0.0.\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.out_chans = out_chans\n",
    "        self.chans = chans\n",
    "        self.num_pool_layers = num_pool_layers\n",
    "        self.drop_prob = drop_prob\n",
    "        self.lr = lr\n",
    "        self.lr_step_size = lr_step_size\n",
    "        self.lr_gamma = lr_gamma\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "        self.unet = Unet(\n",
    "            in_chans=self.in_chans,\n",
    "            out_chans=self.out_chans,\n",
    "            chans=self.chans,\n",
    "            num_pool_layers=self.num_pool_layers,\n",
    "            drop_prob=self.drop_prob,\n",
    "        )\n",
    "\n",
    "    def forward(self, image):\n",
    "        return self.unet(image.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        output = self(batch.image)\n",
    "        loss = F.l1_loss(output, batch.target)\n",
    "\n",
    "        self.log(\"loss\", loss.detach())\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        output = self(batch.image)\n",
    "        mean = batch.mean.unsqueeze(1).unsqueeze(2)\n",
    "        std = batch.std.unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        return {\n",
    "            \"batch_idx\": batch_idx,\n",
    "            \"fname\": batch.fname,\n",
    "            \"slice_num\": batch.slice_num,\n",
    "            \"max_value\": batch.max_value,\n",
    "            \"output\": output * std + mean,\n",
    "            \"target\": batch.target * std + mean,\n",
    "            \"val_loss\": F.l1_loss(output, batch.target),\n",
    "        }\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        output = self.forward(batch.image)\n",
    "        mean = batch.mean.unsqueeze(1).unsqueeze(2)\n",
    "        std = batch.std.unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        return {\n",
    "            \"fname\": batch.fname,\n",
    "            \"slice\": batch.slice_num,\n",
    "            \"output\": (output * std + mean).cpu().numpy(),\n",
    "        }\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.RMSprop(\n",
    "            self.parameters(),\n",
    "            lr=self.lr,\n",
    "            weight_decay=self.weight_decay,\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "            optim, self.lr_step_size, self.lr_gamma\n",
    "        )\n",
    "\n",
    "        return [optim], [scheduler]\n",
    "\n",
    "    @staticmethod\n",
    "    def add_model_specific_args(parent_parser):  # pragma: no-cover\n",
    "        \"\"\"\n",
    "        Define parameters that only apply to this model\n",
    "        \"\"\"\n",
    "        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n",
    "        parser = MriModule.add_model_specific_args(parser)\n",
    "\n",
    "        # network params\n",
    "        parser.add_argument(\n",
    "            \"--in_chans\", default=1, type=int, help=\"Number of U-Net input channels\"\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--out_chans\", default=1, type=int, help=\"Number of U-Net output chanenls\"\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--chans\", default=1, type=int, help=\"Number of top-level U-Net filters.\"\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--num_pool_layers\",\n",
    "            default=4,\n",
    "            type=int,\n",
    "            help=\"Number of U-Net pooling layers.\",\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--drop_prob\", default=0.0, type=float, help=\"U-Net dropout probability\"\n",
    "        )\n",
    "\n",
    "        # training params (opt)\n",
    "        parser.add_argument(\n",
    "            \"--lr\", default=0.001, type=float, help=\"RMSProp learning rate\"\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--lr_step_size\",\n",
    "            default=40,\n",
    "            type=int,\n",
    "            help=\"Epoch at which to decrease step size\",\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--lr_gamma\", default=0.1, type=float, help=\"Amount to decrease step size\"\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--weight_decay\",\n",
    "            default=0.0,\n",
    "            type=float,\n",
    "            help=\"Strength of weight decay regularization\",\n",
    "        )\n",
    "\n",
    "        return parser\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unet with Attention Gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_chans: int, out_chans: int, drop_prob: float):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.out_chans = out_chans\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(in_chans, out_chans, kernel_size=3, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(out_chans),\n",
    "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "            nn.Dropout2d(drop_prob),\n",
    "            nn.Conv2d(out_chans, out_chans, kernel_size=3, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(out_chans),\n",
    "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "            nn.Dropout2d(drop_prob),\n",
    "        )\n",
    " \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class attention_gate(nn.Module):\n",
    "    \"\"\"SOURCES: https://arxiv.org/pdf/1804.03999.pdf\n",
    "                https://idiotdeveloper.com/attention-unet-in-pytorch/\n",
    "       AG is characterised by a set of parameters Θatt containing: linear transformations Wx ∈ Fl×Fint,\n",
    "       Wg ∈ Fg×Fint, ψ ∈ Fint×1 and bias terms. The linear transformations are computed using\n",
    "       channel-wise 1x1x1 convolutions for the input tensors. In other contexts [33], this is referred to as\n",
    "       vector concatenation-based attention, where the concatenated features x and g are linearly mapped\n",
    "       to a Fint dimensional intermediate space.\"\"\"\n",
    "    def __init__(self, F_g, F_l, F_int):\n",
    "        super().__init__()\n",
    " \n",
    "        self.Wg = nn.Sequential(\n",
    "            nn.Conv2d(F_g, F_int, kernel_size=1, padding=0),\n",
    "            nn.InstanceNorm2d(F_int),\n",
    "        )\n",
    "        self.Wx = nn.Sequential(\n",
    "            nn.Conv2d(F_l, F_int, kernel_size=1, padding=0),\n",
    "            nn.InstanceNorm2d(F_int),\n",
    "        )\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv2d(F_int, 1, kernel_size=1,padding=0),\n",
    "            nn.InstanceNorm2d(1),\n",
    "        )\n",
    "        \n",
    "        self.Sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, g, x):\n",
    "        #g is the output from convolution\n",
    "        #x is the skip connection with corresponding dimensions in the downsample\n",
    "        Wg = self.Wg(g)\n",
    "        Wx = self.Wx(x)\n",
    "        alpha = self.relu(Wg + Wx)\n",
    "        alpha = self.psi(alpha)\n",
    "        alpha = self.Sigmoid(alpha)\n",
    "        return x * alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransposeConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A Transpose Convolutional Block that consists of one convolution transpose\n",
    "    layers followed by instance normalization and LeakyReLU activation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_chans: int, out_chans: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_chans: Number of channels in the input.\n",
    "            out_chans: Number of channels in the output.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.out_chans = out_chans\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_chans, out_chans, kernel_size=2, stride=2, bias=False\n",
    "            ),\n",
    "            nn.InstanceNorm2d(out_chans),\n",
    "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, image: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image: Input 4D tensor of shape `(N, in_chans, H, W)`.\n",
    "\n",
    "        Returns:\n",
    "            Output tensor of shape `(N, out_chans, H*2, W*2)`.\n",
    "        \"\"\"\n",
    "        return self.layers(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet(nn.Module):\n",
    "    \"\"\"\n",
    "    PyTorch implementation of a U-Net model.\n",
    "\n",
    "    O. Ronneberger, P. Fischer, and Thomas Brox. U-net: Convolutional networks\n",
    "    for biomedical image segmentation. In International Conference on Medical\n",
    "    image computing and computer-assisted intervention, pages 234–241.\n",
    "    Springer, 2015.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_chans: int,\n",
    "        out_chans: int,\n",
    "        chans: int = 32,\n",
    "        num_pool_layers: int = 4,\n",
    "        drop_prob: float = 0.0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_chans: Number of channels in the input to the U-Net model.\n",
    "            out_chans: Number of channels in the output to the U-Net model.\n",
    "            chans: Number of output channels of the first convolution layer.\n",
    "            num_pool_layers: Number of down-sampling and up-sampling layers.\n",
    "            drop_prob: Dropout probability.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.out_chans = out_chans\n",
    "        self.chans = chans\n",
    "        self.num_pool_layers = num_pool_layers\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "        self.down_sample_layers = nn.ModuleList([ConvBlock(in_chans, chans, drop_prob)])\n",
    "        ch = chans\n",
    "        for _ in range(num_pool_layers - 1):\n",
    "            self.down_sample_layers.append(ConvBlock(ch, ch * 2, drop_prob))\n",
    "            ch *= 2\n",
    "        self.conv = ConvBlock(ch, ch * 2, drop_prob)\n",
    "\n",
    "        self.up_conv = nn.ModuleList()\n",
    "        self.up_transpose_conv = nn.ModuleList()\n",
    "        \n",
    "        #setup list of attention gates\n",
    "        self.attention_gates = nn.ModuleList()\n",
    "        \n",
    "        for _ in range(num_pool_layers - 1):\n",
    "            self.up_transpose_conv.append(TransposeConvBlock(ch * 2, ch))\n",
    "            self.up_conv.append(ConvBlock(ch * 2, ch, drop_prob))\n",
    "            \n",
    "            #append attention_gates into list\n",
    "            self.attention_gates.append(attention_gate(ch, ch, ch//2))\n",
    "            ch //= 2\n",
    "\n",
    "        self.up_transpose_conv.append(TransposeConvBlock(ch * 2, ch))\n",
    "        self.up_conv.append(\n",
    "            nn.Sequential(\n",
    "                ConvBlock(ch * 2, ch, drop_prob),\n",
    "                nn.Conv2d(ch, self.out_chans, kernel_size=1, stride=1),\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        #append one more attention_gate into list\n",
    "        self.attention_gates.append(attention_gate(ch, ch, ch//2))\n",
    "        \n",
    "    def forward(self, image: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image: Input 4D tensor of shape `(N, in_chans, H, W)`.\n",
    "\n",
    "        Returns:\n",
    "            Output tensor of shape `(N, out_chans, H, W)`.\n",
    "        \"\"\"\n",
    "        stack = []\n",
    "        output = image\n",
    "\n",
    "        # apply down-sampling layers\n",
    "        for layer in self.down_sample_layers:\n",
    "            output = layer(output)\n",
    "            stack.append(output)\n",
    "            output = F.avg_pool2d(output, kernel_size=2, stride=2, padding=0)\n",
    "\n",
    "        output = self.conv(output)\n",
    "\n",
    "        # apply up-sampling layers\n",
    "        for transpose_conv, conv, attention in zip(self.up_transpose_conv, self.up_conv, self.attention_gates):\n",
    "            downsample_layer = stack.pop()\n",
    "            output = transpose_conv(output)\n",
    "            \n",
    "            #calculate attention using the output from previous layer and skip connection layer\n",
    "            downsample_layer = attention(output, downsample_layer)\n",
    "            \n",
    "            # reflect pad on the right/botton if needed to handle odd input dimensions\n",
    "            padding = [0, 0, 0, 0]\n",
    "            if output.shape[-1] != downsample_layer.shape[-1]:\n",
    "                padding[1] = 1  # padding right\n",
    "            if output.shape[-2] != downsample_layer.shape[-2]:\n",
    "                padding[3] = 1  # padding bottom\n",
    "            if torch.sum(torch.tensor(padding)) != 0:\n",
    "                output = F.pad(output, padding, \"reflect\")\n",
    "            \n",
    "            output = torch.cat([output, downsample_layer], dim=1)\n",
    "            output = conv(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Space Mask for transforming the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_types = [\n",
    "    \"random\",\n",
    "    \"equispaced\",\n",
    "    \"equispaced_fraction\",\n",
    "    \"magic\",\n",
    "    \"magic_fraction\"\n",
    "]\n",
    "mask_type = mask_types[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of center lines to use in mask\n",
    "center_fractions = [0.09]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acceleration rates to use for masks\n",
    "accelerations = [4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fastmri.data.subsample.RandomMaskFunc"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = create_mask_for_mask_type(\n",
    "    mask_type, center_fractions, accelerations\n",
    ")\n",
    "type(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data specific Parameters\n",
    "data_path = Path('../data/')\n",
    "test_path = Path('../data/singlecoil_test')\n",
    "challenge = \"singlecoil\"\n",
    "test_split = \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fraction of slices in the dataset to use (train split only). \n",
    "# If not given all will be used. Cannot set together with volume_sample_rate.\n",
    "sample_rate = None\n",
    "val_sample_rate = None\n",
    "test_sample_rate = None\n",
    "volume_sample_rate = None\n",
    "val_volume_sample_rate = None\n",
    "test_volume_sample_rate = None\n",
    "use_dataset_cache_file = True\n",
    "combine_train_val = False\n",
    "\n",
    "# data loader arguments\n",
    "batch_size = 1\n",
    "num_workers = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use random masks for train transform, fixed masks for val transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<fastmri.data.transforms.UnetDataTransform at 0x7fbc6b8f5e70>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_transform = UnetDataTransform(challenge, mask_func=mask, use_seed=False)\n",
    "train_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_transform = UnetDataTransform(challenge, mask_func=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform = UnetDataTransform(challenge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'singlecoil'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_module = FastMriDataModule(\n",
    "        data_path=data_path,\n",
    "        challenge=challenge,\n",
    "        train_transform=train_transform,\n",
    "        val_transform=val_transform,\n",
    "        test_transform=test_transform,\n",
    "        test_split=test_split,\n",
    "        test_path=test_path,\n",
    "        sample_rate=sample_rate,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        distributed_sampler=None,\n",
    ")\n",
    "data_module.challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify access to datasets is ready...\n",
    "data_module.prepare_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "# UNet Model Hyperparameters #\n",
    "##############################\n",
    "in_chans=1          # number of input channels to U-Net\n",
    "out_chans=1         # number of output chanenls to U-Net\n",
    "chans=32            # number of top-level U-Net channels\n",
    "num_pool_layers=4   # number of U-Net pooling layers\n",
    "drop_prob=0.0       # dropout probability\n",
    "lr=0.001            # RMSProp learning rate\n",
    "lr_step_size=40     # epoch at which to decrease learning rate\n",
    "lr_gamma=0.1        # extent to which to decrease learning rate\n",
    "weight_decay=0.0    # weight decay regularization strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UnetModule(\n",
    "        in_chans=in_chans,\n",
    "        out_chans=out_chans,\n",
    "        chans=chans,\n",
    "        num_pool_layers=num_pool_layers,\n",
    "        drop_prob=drop_prob,\n",
    "        lr=lr,\n",
    "        lr_step_size=lr_step_size,\n",
    "        lr_gamma=lr_gamma,\n",
    "        weight_decay=weight_decay,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_config = dict(\n",
    "    #replace_sampler_ddp=False,  # this is necessary for volume dispatch during val\n",
    "    #strategy=\"ddp\",               # what distributed version to use\n",
    "    #seed=42,   # random seed\n",
    "    accelerator = \"cpu\",\n",
    "    devices=1,                     # number of gpus to use\n",
    "    deterministic=True,         # makes things slower, but deterministic\n",
    "    default_root_dir='../logs',  # directory for logs and checkpoints\n",
    "    max_epochs=10,              # max number of epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(**trainer_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name             | Type                 | Params\n",
      "----------------------------------------------------------\n",
      "0 | NMSE             | DistributedMetricSum | 0     \n",
      "1 | SSIM             | DistributedMetricSum | 0     \n",
      "2 | PSNR             | DistributedMetricSum | 0     \n",
      "3 | ValLoss          | DistributedMetricSum | 0     \n",
      "4 | TotExamples      | DistributedMetricSum | 0     \n",
      "5 | TotSliceExamples | DistributedMetricSum | 0     \n",
      "6 | unet             | Unet                 | 7.8 M \n",
      "----------------------------------------------------------\n",
      "7.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "7.8 M     Total params\n",
      "31.375    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lrobaina/school/IMPACT-MRI/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/data_loading.py:110: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 87 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/lrobaina/school/IMPACT-MRI/.venv/lib/python3.10/site-packages/pytorch_lightning/utilities/data.py:56: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n",
      "/tmp/ipykernel_31798/1579936338.py:87: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1, 320, 320])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  \"val_loss\": F.l1_loss(output, batch.target),\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input images must have the same dimensions.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(model, datamodule\u001b[39m=\u001b[39;49mdata_module)\n",
      "File \u001b[0;32m~/school/IMPACT-MRI/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:735\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, train_dataloader, ckpt_path)\u001b[0m\n\u001b[1;32m    730\u001b[0m     rank_zero_deprecation(\n\u001b[1;32m    731\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`trainer.fit(train_dataloader)` is deprecated in v1.4 and will be removed in v1.6.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    732\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m Use `trainer.fit(train_dataloaders)` instead. HINT: added \u001b[39m\u001b[39m'\u001b[39m\u001b[39ms\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    733\u001b[0m     )\n\u001b[1;32m    734\u001b[0m     train_dataloaders \u001b[39m=\u001b[39m train_dataloader\n\u001b[0;32m--> 735\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    736\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    737\u001b[0m )\n",
      "File \u001b[0;32m~/school/IMPACT-MRI/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:682\u001b[0m, in \u001b[0;36mTrainer._call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    673\u001b[0m \u001b[39mError handling, intended to be used only for main trainer function entry points (fit, validate, test, predict)\u001b[39;00m\n\u001b[1;32m    674\u001b[0m \u001b[39mas all errors should funnel through them\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[39m    **kwargs: keyword arguments to be passed to `trainer_fn`\u001b[39;00m\n\u001b[1;32m    680\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    681\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 682\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    683\u001b[0m \u001b[39m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001b[39;00m\n\u001b[1;32m    684\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m \u001b[39mas\u001b[39;00m exception:\n",
      "File \u001b[0;32m~/school/IMPACT-MRI/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:770\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    768\u001b[0m \u001b[39m# TODO: ckpt_path only in v1.7\u001b[39;00m\n\u001b[1;32m    769\u001b[0m ckpt_path \u001b[39m=\u001b[39m ckpt_path \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresume_from_checkpoint\n\u001b[0;32m--> 770\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[1;32m    772\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    773\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/school/IMPACT-MRI/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1193\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheckpoint_connector\u001b[39m.\u001b[39mresume_end()\n\u001b[1;32m   1192\u001b[0m \u001b[39m# dispatch `start_training` or `start_evaluating` or `start_predicting`\u001b[39;00m\n\u001b[0;32m-> 1193\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch()\n\u001b[1;32m   1195\u001b[0m \u001b[39m# plugin will finalized fitting (e.g. ddp_spawn will load trained model)\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_post_dispatch()\n",
      "File \u001b[0;32m~/school/IMPACT-MRI/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1272\u001b[0m, in \u001b[0;36mTrainer._dispatch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_type_plugin\u001b[39m.\u001b[39mstart_predicting(\u001b[39mself\u001b[39m)\n\u001b[1;32m   1271\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1272\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_type_plugin\u001b[39m.\u001b[39;49mstart_training(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[0;32m~/school/IMPACT-MRI/.venv/lib/python3.10/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py:202\u001b[0m, in \u001b[0;36mTrainingTypePlugin.start_training\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstart_training\u001b[39m(\u001b[39mself\u001b[39m, trainer: \u001b[39m\"\u001b[39m\u001b[39mpl.Trainer\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m     \u001b[39m# double dispatch to initiate the training loop\u001b[39;00m\n\u001b[0;32m--> 202\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_results \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mrun_stage()\n",
      "File \u001b[0;32m~/school/IMPACT-MRI/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1282\u001b[0m, in \u001b[0;36mTrainer.run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1280\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[1;32m   1281\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_predict()\n\u001b[0;32m-> 1282\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_train()\n",
      "File \u001b[0;32m~/school/IMPACT-MRI/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1304\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1301\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_global_zero \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprogress_bar_callback \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1302\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprogress_bar_callback\u001b[39m.\u001b[39mdisable()\n\u001b[0;32m-> 1304\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_sanity_check(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlightning_module)\n\u001b[1;32m   1306\u001b[0m \u001b[39m# enable train mode\u001b[39;00m\n\u001b[1;32m   1307\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/school/IMPACT-MRI/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1368\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self, ref_model)\u001b[0m\n\u001b[1;32m   1366\u001b[0m \u001b[39m# run eval step\u001b[39;00m\n\u001b[1;32m   1367\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m-> 1368\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluation_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m   1370\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcall_hook(\u001b[39m\"\u001b[39m\u001b[39mon_sanity_check_end\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1372\u001b[0m \u001b[39m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m~/school/IMPACT-MRI/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py:145\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 145\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    146\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    147\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrestarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/school/IMPACT-MRI/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py:109\u001b[0m, in \u001b[0;36mEvaluationLoop.advance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_fetcher \u001b[39m=\u001b[39m dataloader \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39m_data_connector\u001b[39m.\u001b[39mget_profiled_dataloader(\n\u001b[1;32m    105\u001b[0m     dataloader, dataloader_idx\u001b[39m=\u001b[39mdataloader_idx\n\u001b[1;32m    106\u001b[0m )\n\u001b[1;32m    107\u001b[0m dl_max_batches \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_batches[dataloader_idx]\n\u001b[0;32m--> 109\u001b[0m dl_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(dataloader, dataloader_idx, dl_max_batches, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_dataloaders)\n\u001b[1;32m    111\u001b[0m \u001b[39m# store batch level output per dataloader\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutputs\u001b[39m.\u001b[39mappend(dl_outputs)\n",
      "File \u001b[0;32m~/school/IMPACT-MRI/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py:145\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 145\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    146\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    147\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrestarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/school/IMPACT-MRI/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:124\u001b[0m, in \u001b[0;36mEvaluationEpochLoop.advance\u001b[0;34m(self, data_fetcher, dataloader_idx, dl_max_batches, num_dataloaders)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mevaluation_step_and_end\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    123\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_evaluation_step(batch, batch_idx, dataloader_idx)\n\u001b[0;32m--> 124\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluation_step_end(output)\n\u001b[1;32m    126\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_processed()\n\u001b[1;32m    128\u001b[0m \u001b[39m# track loss history\u001b[39;00m\n",
      "File \u001b[0;32m~/school/IMPACT-MRI/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:222\u001b[0m, in \u001b[0;36mEvaluationEpochLoop._evaluation_step_end\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Calls the `{validation/test}_step_end` hook.\"\"\"\u001b[39;00m\n\u001b[1;32m    221\u001b[0m hook_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtest_step_end\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mtesting \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mvalidation_step_end\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 222\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mcall_hook(hook_name, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    223\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/school/IMPACT-MRI/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1483\u001b[0m, in \u001b[0;36mTrainer.call_hook\u001b[0;34m(self, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1481\u001b[0m model_fx \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(pl_module, hook_name, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m   1482\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcallable\u001b[39m(model_fx):\n\u001b[0;32m-> 1483\u001b[0m     output \u001b[39m=\u001b[39m model_fx(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1485\u001b[0m \u001b[39m# *Bad code alert*\u001b[39;00m\n\u001b[1;32m   1486\u001b[0m \u001b[39m# The `Accelerator` mostly calls the `TrainingTypePlugin` but some of those calls are deprecated.\u001b[39;00m\n\u001b[1;32m   1487\u001b[0m \u001b[39m# The following logic selectively chooses which hooks are called on each object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1491\u001b[0m \n\u001b[1;32m   1492\u001b[0m \u001b[39m# call the accelerator hook\u001b[39;00m\n\u001b[1;32m   1493\u001b[0m \u001b[39mif\u001b[39;00m hook_name \u001b[39min\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39mon_train_start\u001b[39m\u001b[39m\"\u001b[39m,) \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator, hook_name):\n",
      "Cell \u001b[0;32mIn[2], line 119\u001b[0m, in \u001b[0;36mMriModule.validation_step_end\u001b[0;34m(self, val_logs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     mse_vals[fname][slice_num] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(\n\u001b[1;32m    113\u001b[0m         evaluate\u001b[39m.\u001b[39mmse(target, output)\n\u001b[1;32m    114\u001b[0m     )\u001b[39m.\u001b[39mview(\u001b[39m1\u001b[39m)\n\u001b[1;32m    115\u001b[0m     target_norms[fname][slice_num] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(\n\u001b[1;32m    116\u001b[0m         evaluate\u001b[39m.\u001b[39mmse(target, np\u001b[39m.\u001b[39mzeros_like(target))\n\u001b[1;32m    117\u001b[0m     )\u001b[39m.\u001b[39mview(\u001b[39m1\u001b[39m)\n\u001b[1;32m    118\u001b[0m     ssim_vals[fname][slice_num] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(\n\u001b[0;32m--> 119\u001b[0m         evaluate\u001b[39m.\u001b[39;49mssim(target[\u001b[39mNone\u001b[39;49;00m, \u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m], output[\u001b[39mNone\u001b[39;49;00m, \u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m], maxval\u001b[39m=\u001b[39;49mmaxval)\n\u001b[1;32m    120\u001b[0m     )\u001b[39m.\u001b[39mview(\u001b[39m1\u001b[39m)\n\u001b[1;32m    121\u001b[0m     max_vals[fname] \u001b[39m=\u001b[39m maxval\n\u001b[1;32m    123\u001b[0m \u001b[39mreturn\u001b[39;00m {\n\u001b[1;32m    124\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m\"\u001b[39m: val_logs[\u001b[39m\"\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    125\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mmse_vals\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mdict\u001b[39m(mse_vals),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mmax_vals\u001b[39m\u001b[39m\"\u001b[39m: max_vals,\n\u001b[1;32m    129\u001b[0m }\n",
      "File \u001b[0;32m~/school/IMPACT-MRI/fastmri/evaluate.py:53\u001b[0m, in \u001b[0;36mssim\u001b[0;34m(gt, pred, maxval)\u001b[0m\n\u001b[1;32m     51\u001b[0m ssim \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([\u001b[39m0\u001b[39m])\n\u001b[1;32m     52\u001b[0m \u001b[39mfor\u001b[39;00m slice_num \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(gt\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]):\n\u001b[0;32m---> 53\u001b[0m     ssim \u001b[39m=\u001b[39m ssim \u001b[39m+\u001b[39m structural_similarity(\n\u001b[1;32m     54\u001b[0m         gt[slice_num], pred[slice_num], data_range\u001b[39m=\u001b[39;49mmaxval\n\u001b[1;32m     55\u001b[0m     )\n\u001b[1;32m     57\u001b[0m \u001b[39mreturn\u001b[39;00m ssim \u001b[39m/\u001b[39m gt\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/school/IMPACT-MRI/.venv/lib/python3.10/site-packages/skimage/metrics/_structural_similarity.py:111\u001b[0m, in \u001b[0;36mstructural_similarity\u001b[0;34m(im1, im2, win_size, gradient, data_range, channel_axis, gaussian_weights, full, **kwargs)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstructural_similarity\u001b[39m(im1, im2,\n\u001b[1;32m     16\u001b[0m                           \u001b[39m*\u001b[39m,\n\u001b[1;32m     17\u001b[0m                           win_size\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, gradient\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, data_range\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     18\u001b[0m                           channel_axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     19\u001b[0m                           gaussian_weights\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, full\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     20\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[39m    Compute the mean structural similarity index between two images.\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[39m    Please pay attention to the `data_range` parameter with floating-point images.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    109\u001b[0m \n\u001b[1;32m    110\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m     check_shape_equality(im1, im2)\n\u001b[1;32m    112\u001b[0m     float_type \u001b[39m=\u001b[39m _supported_float_type(im1\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m    114\u001b[0m     \u001b[39mif\u001b[39;00m channel_axis \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    115\u001b[0m         \u001b[39m# loop over channels\u001b[39;00m\n",
      "File \u001b[0;32m~/school/IMPACT-MRI/.venv/lib/python3.10/site-packages/skimage/_shared/utils.py:504\u001b[0m, in \u001b[0;36mcheck_shape_equality\u001b[0;34m(*images)\u001b[0m\n\u001b[1;32m    502\u001b[0m image0 \u001b[39m=\u001b[39m images[\u001b[39m0\u001b[39m]\n\u001b[1;32m    503\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mall\u001b[39m(image0\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m image\u001b[39m.\u001b[39mshape \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images[\u001b[39m1\u001b[39m:]):\n\u001b[0;32m--> 504\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mInput images must have the same dimensions.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    505\u001b[0m \u001b[39mreturn\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Input images must have the same dimensions."
     ]
    }
   ],
   "source": [
    "trainer.fit(model, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
